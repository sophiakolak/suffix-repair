{"project": "scrapy", "bug": 1, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/spidermiddlewares/offsite.py", "buggy_commit_id": "c57512fa669e6f6b1b766a7639206a380f0d10ce", "fixed_commit_id": "9d9dea0d69709ef0f7aef67ddba1bd7bda25d273", "lines_deleted": {"57": "            if url_pattern.match(domain):\n", "61": "        domains = [re.escape(d) for d in allowed_domains if d is not None]\n"}, "lines_added": {"56": "        domains = []\n", "58": "            if domain is None:\n", "59": "                continue\n", "60": "            elif url_pattern.match(domain):\n", "64": "            else:\n", "65": "                domains.append(re.escape(domain))\n"}}
{"project": "scrapy", "bug": 2, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/datatypes.py", "buggy_commit_id": "f02c3d1dcf3e4880388d19e961e7911be5dc54ff", "fixed_commit_id": "439a3e59b8e858441f8d97dbc32f398db392330d", "lines_deleted": {"317": "        while len(self) >= self.limit:\n", "318": "            self.popitem(last=False)\n"}, "lines_added": {"317": "        if self.limit:\n", "318": "            while len(self) >= self.limit:\n", "319": "                self.popitem(last=False)\n"}}
{"project": "scrapy", "bug": 4, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/contracts/__init__.py", "buggy_commit_id": "c8f3d07e86dd41074971b5423fb932c2eda6db1e", "fixed_commit_id": "16dad81715d3970149c0cf7a318e73a0d84be1ff", "lines_deleted": {"86": "            exc_info = failure.value, failure.type, failure.getTracebackObject()\n"}, "lines_added": {"86": "            exc_info = failure.type, failure.value, failure.getTracebackObject()\n"}}
{"project": "scrapy", "bug": 5, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/response/__init__.py", "buggy_commit_id": "426da0ed07637e7efbcfb0fe49546e187d5d7f67", "fixed_commit_id": "acd2b8d43b5ebec7ffd364b6f335427041a0b98d", "lines_deleted": {}, "lines_added": {"122": "        elif url is None:\n", "123": "            raise ValueError(\"url can't be None\")\n"}}
{"project": "scrapy", "bug": 6, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/pipelines/images.py", "buggy_commit_id": "8aa2e4f9976d31bbe3a0014b4f1f96ace1b87043", "fixed_commit_id": "25f609e2a3c27ca7d7d98dbfddb2c049735935bb", "lines_deleted": {}, "lines_added": {"134": "        elif image.mode == 'P':\n", "135": "            image = image.convert(\"RGBA\")\n", "136": "            background = Image.new('RGBA', image.size, (255, 255, 255))\n", "137": "            background.paste(image, image)\n", "138": "            image = background.convert('RGB')\n"}}
{"project": "scrapy", "bug": 11, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/gz.py", "buggy_commit_id": "241bd00e76df142a24699819f8496bdec8f5c83a", "fixed_commit_id": "9de6f1ca757b7f200d15e94840c9d431cf202276", "lines_deleted": {"45": "                    output += f.extrabuf\n"}, "lines_added": {"45": "                    output += f.extrabuf[-f.extrasize:]\n"}}
{"project": "scrapy", "bug": 12, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/selector/unified.py", "buggy_commit_id": "34e7dadf38ba1796094c0c76e92ea8d9837681cc", "fixed_commit_id": "2c9a38d1f54a12c33d7c9a19e021c840c4a32dee", "lines_deleted": {}, "lines_added": {"48": "        if not(response is None or text is None):\n", "49": "           raise ValueError('%s.__init__() received both response and text'\n", "50": "                            % self.__class__.__name__)\n", "51": "\n"}}
{"project": "scrapy", "bug": 13, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/pipelines/images.py", "buggy_commit_id": "fa78849e335994a1617ed63221a70940c21cca20", "fixed_commit_id": "414857a593ad5b82fa21d6344928f43f93dc9f14", "lines_deleted": {"44": "    EXPIRES = 0\n"}, "lines_added": {"44": "    EXPIRES = 90\n"}}
{"project": "scrapy", "bug": 15, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/url.py", "buggy_commit_id": "b7925e42202d79d2ba9d00b6aded3a451c92fe81", "fixed_commit_id": "1aec5200bc81493623f2a4e077b4e80e104e47d5", "lines_deleted": {"45": "        to_native_str(parts.netloc.encode('idna')),\n"}, "lines_added": {"43": "    # IDNA encoding can fail for too long labels (>63 characters)\n", "44": "    # or missing labels (e.g. http://.example.com)\n", "45": "    try:\n", "46": "        netloc = parts.netloc.encode('idna')\n", "47": "    except UnicodeError:\n", "48": "        netloc = parts.netloc\n", "49": "\n", "52": "        to_native_str(netloc),\n"}}
{"project": "scrapy", "bug": 17, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/response.py", "buggy_commit_id": "ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16", "fixed_commit_id": "65c7c05060fd2d1fc161d4904243d5e0b31e202b", "lines_deleted": {"49": "\n", "50": "    >>> response_status_message(200)\n", "51": "    '200 OK'\n", "52": "\n", "53": "    >>> response_status_message(404)\n", "54": "    '404 Not Found'\n", "56": "    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))\n"}, "lines_added": {"50": "    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), \"Unknown Status\")))\n"}}
{"project": "scrapy", "bug": 18, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/responsetypes.py", "buggy_commit_id": "41588397c04356f2b0c393b61ed68271a08d6ccd", "fixed_commit_id": "cabed6f183cfb2ab778c57be8c75802fec5e54d4", "lines_deleted": {"61": "            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n"}, "lines_added": {"61": "            filename = to_native_str(content_disposition,\n", "62": "                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n"}}
{"project": "scrapy", "bug": 19, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/cookies.py", "buggy_commit_id": "e328a9b9dfa4fbc79c59ed4f45f757e998301c31", "fixed_commit_id": "1f743996ff00a7b728d59b93d0967e1eb50072f0", "lines_deleted": {"139": "    # python3 uses request.unverifiable\n", "144": "    def get_origin_req_host(self):\n", "145": "        return urlparse_cached(self.request).hostname\n"}, "lines_added": {"139": "    def get_origin_req_host(self):\n", "140": "        return urlparse_cached(self.request).hostname\n", "141": "\n", "142": "    # python3 uses attributes instead of methods\n", "143": "    @property\n", "144": "    def full_url(self):\n", "145": "        return self.get_full_url()\n", "146": "\n", "147": "    @property\n", "148": "    def host(self):\n", "149": "        return self.get_host()\n", "150": "\n", "151": "    @property\n", "152": "    def type(self):\n", "153": "        return self.get_type()\n", "154": "\n", "155": "    @property\n", "160": "    def origin_req_host(self):\n", "161": "        return self.get_origin_req_host()\n"}}
{"project": "scrapy", "bug": 20, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/spiders/sitemap.py", "buggy_commit_id": "e328a9b9dfa4fbc79c59ed4f45f757e998301c31", "fixed_commit_id": "25c56159b86288311630cc0cf6db9d755aeeff1e", "lines_deleted": {"34": "            for url in sitemap_urls_from_robots(response.body):\n"}, "lines_added": {"34": "            for url in sitemap_urls_from_robots(response.text):\n"}}
{"project": "scrapy", "bug": 21, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/downloadermiddlewares/robotstxt.py", "buggy_commit_id": "43a53aca1207a82b663fe7a90c375546ce340a8e", "fixed_commit_id": "a8a6f050e71fbb7881076a8d6e2867e868d26016", "lines_deleted": {"103": "        self._parsers.pop(netloc).callback(None)\n"}, "lines_added": {"103": "        rp_dfd = self._parsers[netloc]\n", "104": "        self._parsers[netloc] = None\n", "105": "        rp_dfd.callback(None)\n"}}
{"project": "scrapy", "bug": 22, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/exporters.py", "buggy_commit_id": "a35aec71e96b0c0288c370afa425e8e700dca8b3", "fixed_commit_id": "bb2cf7c0d7199fffe0aa100e5c8a51c6b4b82fc2", "lines_deleted": {"146": "        else:\n"}, "lines_added": {"146": "        elif isinstance(serialized_value, six.text_type):\n", "148": "        else:\n", "149": "            self._xg_characters(str(serialized_value))\n"}}
{"project": "scrapy", "bug": 27, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/downloadermiddlewares/redirect.py", "buggy_commit_id": "280eab241680c93a763a3ef3a9ccd0c257259ca0", "fixed_commit_id": "d164398a27736f75286cc435eca69b06ff7c1c06", "lines_deleted": {"57": "               response.status in getattr(spider, 'handle_httpstatus_list', [])):\n"}, "lines_added": {"57": "               response.status in getattr(spider, 'handle_httpstatus_list', []) or\n", "58": "               response.status in request.meta.get('handle_httpstatus_list', []) or\n", "59": "               request.meta.get('handle_httpstatus_all', False)):\n"}}
{"project": "scrapy", "bug": 28, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/dupefilters.py", "buggy_commit_id": "e2f31f3018c0037f65982209c22f93b80a5d6e7b", "fixed_commit_id": "457b97c13ccf9a84f3dc7800c180cf059822c09a", "lines_deleted": {}, "lines_added": {"38": "            self.file.seek(0)\n"}}
{"project": "scrapy", "bug": 29, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/request.py", "buggy_commit_id": "5c4666a3d489bd3efa2e188de58721a125a5bfad", "fixed_commit_id": "8d45b3c4810cb5304ba1193b45697a0df1157326", "lines_deleted": {"81": "    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n"}, "lines_added": {"81": "    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n"}}
{"project": "scrapy", "bug": 32, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/crawler.py", "buggy_commit_id": "342cb622f1ea93268477da557099010bbd72529a", "fixed_commit_id": "aa6a72707daabfb6217f52e4774f2ff038f83dcc", "lines_deleted": {"211": "        configure_logging(settings)\n", "212": "        log_scrapy_info(settings)\n"}, "lines_added": {"211": "        configure_logging(self.settings)\n", "212": "        log_scrapy_info(self.settings)\n"}}
{"project": "scrapy", "bug": 35, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/crawler.py", "buggy_commit_id": "0a5bbbaed3e182d2151b6b34357667901ece353f", "fixed_commit_id": "c3d3a9491412d2a91b0927a05908593dcd329e4a", "lines_deleted": {"194": "    cls_path = settings.get('SPIDER_LOADER_CLASS',\n", "195": "                            settings.get('SPIDER_MANAGER_CLASS'))\n"}, "lines_added": {"194": "    cls_path = settings.get('SPIDER_MANAGER_CLASS',\n", "195": "                            settings.get('SPIDER_LOADER_CLASS'))\n"}}
{"project": "scrapy", "bug": 36, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/misc.py", "buggy_commit_id": "cb8140a42a27ede87b0880372024f2f1804618b8", "fixed_commit_id": "cf9be5344a89dd8e14f8241ec69de9c984ec1e05", "lines_deleted": {"145": "        return objcls.from_crawler(crawler, *args, **kwargs)\n", "147": "        return objcls.from_settings(settings, *args, **kwargs)\n", "149": "        return objcls(*args, **kwargs)\n"}, "lines_added": {"137": "\n", "140": "    Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n", "141": "    extension has not been implemented correctly).\n", "148": "        instance = objcls.from_crawler(crawler, *args, **kwargs)\n", "149": "        method_name = 'from_crawler'\n", "151": "        instance = objcls.from_settings(settings, *args, **kwargs)\n", "152": "        method_name = 'from_settings'\n", "154": "        instance = objcls(*args, **kwargs)\n", "155": "        method_name = '__new__'\n", "156": "    if instance is None:\n", "157": "        raise TypeError(\"%s.%s returned None\" % (objcls.__qualname__, method_name))\n", "158": "    return instance\n"}}
{"project": "scrapy", "bug": 37, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/request/__init__.py", "buggy_commit_id": "1d5c270ce8caf954ce83c8db262e2a35707e0c5e", "fixed_commit_id": "f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a", "lines_deleted": {"68": "        if ':' not in self._url:\n"}, "lines_added": {"68": "        if ('://' not in self._url) and (not self._url.startswith('data:')):\n"}}
{"project": "scrapy", "bug": 38, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/request/form.py", "buggy_commit_id": "6cc6bbb5fc5c102271829a554772effb0444023c", "fixed_commit_id": "6c3970e6722191b642fd99c6c1bfed0d93010cab", "lines_deleted": {"172": "            'descendant::*[(self::input or self::button)'\n", "173": "            ' and re:test(@type, \"^submit$\", \"i\")]'\n", "174": "            '|descendant::button[not(@type)]',\n"}, "lines_added": {"172": "            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n", "173": "            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n"}}
{"project": "scrapy", "bug": 39, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/spiders/__init__.py", "buggy_commit_id": "692975acb40c6394424dfb728b1ffa46b3b3c55d", "fixed_commit_id": "a1e8a8525d2312842c7e1cca8ba6e4e1a83084b7", "lines_deleted": {"68": "        if self.make_requests_from_url is not Spider.make_requests_from_url:\n", "70": "                \"Spider.make_requests_from_url method is deprecated; \"\n", "71": "                \"it won't be called in future Scrapy releases. \"\n", "72": "                \"Please override start_requests method instead.\"\n"}, "lines_added": {"68": "        cls = self.__class__\n", "69": "        if cls.make_requests_from_url is not Spider.make_requests_from_url:\n", "71": "                \"Spider.make_requests_from_url method is deprecated; it \"\n", "72": "                \"won't be called in future Scrapy releases. Please \"\n", "73": "                \"override Spider.start_requests method instead (see %s.%s).\" % (\n", "74": "                    cls.__module__, cls.__name__\n", "75": "                ),\n"}}
{"project": "scrapy", "bug": 40, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/exporters.py", "buggy_commit_id": "7d24df37380cd5a5b7394cd2534e240bd2eff0ca", "fixed_commit_id": "f1d971a5c0cdfe0f4fe5619146cd6818324fc98e", "lines_deleted": {"275": "        if self.binary:\n", "276": "            return to_bytes(value, encoding=self.encoding)\n", "277": "        else:\n", "278": "            return to_unicode(value, encoding=self.encoding)\n"}, "lines_added": {"275": "        encode_func = to_bytes if self.binary else to_unicode\n", "276": "        if isinstance(value, (six.text_type, bytes)):\n", "277": "            return encode_func(value, encoding=self.encoding)\n", "278": "        return value\n"}}
{"project": "scrapy", "bug": 1, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/spidermiddlewares/offsite.py", "buggy_commit_id": "c57512fa669e6f6b1b766a7639206a380f0d10ce", "fixed_commit_id": "9d9dea0d69709ef0f7aef67ddba1bd7bda25d273", "lines_deleted": {"57": "            if url_pattern.match(domain):\n", "61": "        domains = [re.escape(d) for d in allowed_domains if d is not None]\n"}, "lines_added": {"56": "        domains = []\n", "58": "            if domain is None:\n", "59": "                continue\n", "60": "            elif url_pattern.match(domain):\n", "64": "            else:\n", "65": "                domains.append(re.escape(domain))\n"}}
{"project": "scrapy", "bug": 2, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/datatypes.py", "buggy_commit_id": "f02c3d1dcf3e4880388d19e961e7911be5dc54ff", "fixed_commit_id": "439a3e59b8e858441f8d97dbc32f398db392330d", "lines_deleted": {"317": "        while len(self) >= self.limit:\n", "318": "            self.popitem(last=False)\n"}, "lines_added": {"317": "        if self.limit:\n", "318": "            while len(self) >= self.limit:\n", "319": "                self.popitem(last=False)\n"}}
{"project": "scrapy", "bug": 4, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/contracts/__init__.py", "buggy_commit_id": "c8f3d07e86dd41074971b5423fb932c2eda6db1e", "fixed_commit_id": "16dad81715d3970149c0cf7a318e73a0d84be1ff", "lines_deleted": {"86": "            exc_info = failure.value, failure.type, failure.getTracebackObject()\n"}, "lines_added": {"86": "            exc_info = failure.type, failure.value, failure.getTracebackObject()\n"}}
{"project": "scrapy", "bug": 5, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/response/__init__.py", "buggy_commit_id": "426da0ed07637e7efbcfb0fe49546e187d5d7f67", "fixed_commit_id": "acd2b8d43b5ebec7ffd364b6f335427041a0b98d", "lines_deleted": {}, "lines_added": {"122": "        elif url is None:\n", "123": "            raise ValueError(\"url can't be None\")\n"}}
{"project": "scrapy", "bug": 6, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/pipelines/images.py", "buggy_commit_id": "8aa2e4f9976d31bbe3a0014b4f1f96ace1b87043", "fixed_commit_id": "25f609e2a3c27ca7d7d98dbfddb2c049735935bb", "lines_deleted": {}, "lines_added": {"134": "        elif image.mode == 'P':\n", "135": "            image = image.convert(\"RGBA\")\n", "136": "            background = Image.new('RGBA', image.size, (255, 255, 255))\n", "137": "            background.paste(image, image)\n", "138": "            image = background.convert('RGB')\n"}}
{"project": "scrapy", "bug": 11, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/gz.py", "buggy_commit_id": "241bd00e76df142a24699819f8496bdec8f5c83a", "fixed_commit_id": "9de6f1ca757b7f200d15e94840c9d431cf202276", "lines_deleted": {"45": "                    output += f.extrabuf\n"}, "lines_added": {"45": "                    output += f.extrabuf[-f.extrasize:]\n"}}
{"project": "scrapy", "bug": 12, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/selector/unified.py", "buggy_commit_id": "34e7dadf38ba1796094c0c76e92ea8d9837681cc", "fixed_commit_id": "2c9a38d1f54a12c33d7c9a19e021c840c4a32dee", "lines_deleted": {}, "lines_added": {"48": "        if not(response is None or text is None):\n", "49": "           raise ValueError('%s.__init__() received both response and text'\n", "50": "                            % self.__class__.__name__)\n", "51": "\n"}}
{"project": "scrapy", "bug": 13, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/pipelines/images.py", "buggy_commit_id": "fa78849e335994a1617ed63221a70940c21cca20", "fixed_commit_id": "414857a593ad5b82fa21d6344928f43f93dc9f14", "lines_deleted": {"44": "    EXPIRES = 0\n"}, "lines_added": {"44": "    EXPIRES = 90\n"}}
{"project": "scrapy", "bug": 15, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/url.py", "buggy_commit_id": "b7925e42202d79d2ba9d00b6aded3a451c92fe81", "fixed_commit_id": "1aec5200bc81493623f2a4e077b4e80e104e47d5", "lines_deleted": {"45": "        to_native_str(parts.netloc.encode('idna')),\n"}, "lines_added": {"43": "    # IDNA encoding can fail for too long labels (>63 characters)\n", "44": "    # or missing labels (e.g. http://.example.com)\n", "45": "    try:\n", "46": "        netloc = parts.netloc.encode('idna')\n", "47": "    except UnicodeError:\n", "48": "        netloc = parts.netloc\n", "49": "\n", "52": "        to_native_str(netloc),\n"}}
{"project": "scrapy", "bug": 17, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/response.py", "buggy_commit_id": "ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16", "fixed_commit_id": "65c7c05060fd2d1fc161d4904243d5e0b31e202b", "lines_deleted": {"49": "\n", "50": "    >>> response_status_message(200)\n", "51": "    '200 OK'\n", "52": "\n", "53": "    >>> response_status_message(404)\n", "54": "    '404 Not Found'\n", "56": "    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))\n"}, "lines_added": {"50": "    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), \"Unknown Status\")))\n"}}
{"project": "scrapy", "bug": 18, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/responsetypes.py", "buggy_commit_id": "41588397c04356f2b0c393b61ed68271a08d6ccd", "fixed_commit_id": "cabed6f183cfb2ab778c57be8c75802fec5e54d4", "lines_deleted": {"61": "            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n"}, "lines_added": {"61": "            filename = to_native_str(content_disposition,\n", "62": "                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n"}}
{"project": "scrapy", "bug": 19, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/cookies.py", "buggy_commit_id": "e328a9b9dfa4fbc79c59ed4f45f757e998301c31", "fixed_commit_id": "1f743996ff00a7b728d59b93d0967e1eb50072f0", "lines_deleted": {"139": "    # python3 uses request.unverifiable\n", "144": "    def get_origin_req_host(self):\n", "145": "        return urlparse_cached(self.request).hostname\n"}, "lines_added": {"139": "    def get_origin_req_host(self):\n", "140": "        return urlparse_cached(self.request).hostname\n", "141": "\n", "142": "    # python3 uses attributes instead of methods\n", "143": "    @property\n", "144": "    def full_url(self):\n", "145": "        return self.get_full_url()\n", "146": "\n", "147": "    @property\n", "148": "    def host(self):\n", "149": "        return self.get_host()\n", "150": "\n", "151": "    @property\n", "152": "    def type(self):\n", "153": "        return self.get_type()\n", "154": "\n", "155": "    @property\n", "160": "    def origin_req_host(self):\n", "161": "        return self.get_origin_req_host()\n"}}
{"project": "scrapy", "bug": 20, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/spiders/sitemap.py", "buggy_commit_id": "e328a9b9dfa4fbc79c59ed4f45f757e998301c31", "fixed_commit_id": "25c56159b86288311630cc0cf6db9d755aeeff1e", "lines_deleted": {"34": "            for url in sitemap_urls_from_robots(response.body):\n"}, "lines_added": {"34": "            for url in sitemap_urls_from_robots(response.text):\n"}}
{"project": "scrapy", "bug": 21, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/downloadermiddlewares/robotstxt.py", "buggy_commit_id": "43a53aca1207a82b663fe7a90c375546ce340a8e", "fixed_commit_id": "a8a6f050e71fbb7881076a8d6e2867e868d26016", "lines_deleted": {"103": "        self._parsers.pop(netloc).callback(None)\n"}, "lines_added": {"103": "        rp_dfd = self._parsers[netloc]\n", "104": "        self._parsers[netloc] = None\n", "105": "        rp_dfd.callback(None)\n"}}
{"project": "scrapy", "bug": 22, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/exporters.py", "buggy_commit_id": "a35aec71e96b0c0288c370afa425e8e700dca8b3", "fixed_commit_id": "bb2cf7c0d7199fffe0aa100e5c8a51c6b4b82fc2", "lines_deleted": {"146": "        else:\n"}, "lines_added": {"146": "        elif isinstance(serialized_value, six.text_type):\n", "148": "        else:\n", "149": "            self._xg_characters(str(serialized_value))\n"}}
{"project": "scrapy", "bug": 27, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/downloadermiddlewares/redirect.py", "buggy_commit_id": "280eab241680c93a763a3ef3a9ccd0c257259ca0", "fixed_commit_id": "d164398a27736f75286cc435eca69b06ff7c1c06", "lines_deleted": {"57": "               response.status in getattr(spider, 'handle_httpstatus_list', [])):\n"}, "lines_added": {"57": "               response.status in getattr(spider, 'handle_httpstatus_list', []) or\n", "58": "               response.status in request.meta.get('handle_httpstatus_list', []) or\n", "59": "               request.meta.get('handle_httpstatus_all', False)):\n"}}
{"project": "scrapy", "bug": 28, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/dupefilters.py", "buggy_commit_id": "e2f31f3018c0037f65982209c22f93b80a5d6e7b", "fixed_commit_id": "457b97c13ccf9a84f3dc7800c180cf059822c09a", "lines_deleted": {}, "lines_added": {"38": "            self.file.seek(0)\n"}}
{"project": "scrapy", "bug": 29, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/request.py", "buggy_commit_id": "5c4666a3d489bd3efa2e188de58721a125a5bfad", "fixed_commit_id": "8d45b3c4810cb5304ba1193b45697a0df1157326", "lines_deleted": {"81": "    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n"}, "lines_added": {"81": "    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n"}}
{"project": "scrapy", "bug": 32, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/crawler.py", "buggy_commit_id": "342cb622f1ea93268477da557099010bbd72529a", "fixed_commit_id": "aa6a72707daabfb6217f52e4774f2ff038f83dcc", "lines_deleted": {"211": "        configure_logging(settings)\n", "212": "        log_scrapy_info(settings)\n"}, "lines_added": {"211": "        configure_logging(self.settings)\n", "212": "        log_scrapy_info(self.settings)\n"}}
{"project": "scrapy", "bug": 35, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/crawler.py", "buggy_commit_id": "0a5bbbaed3e182d2151b6b34357667901ece353f", "fixed_commit_id": "c3d3a9491412d2a91b0927a05908593dcd329e4a", "lines_deleted": {"194": "    cls_path = settings.get('SPIDER_LOADER_CLASS',\n", "195": "                            settings.get('SPIDER_MANAGER_CLASS'))\n"}, "lines_added": {"194": "    cls_path = settings.get('SPIDER_MANAGER_CLASS',\n", "195": "                            settings.get('SPIDER_LOADER_CLASS'))\n"}}
{"project": "scrapy", "bug": 36, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/misc.py", "buggy_commit_id": "cb8140a42a27ede87b0880372024f2f1804618b8", "fixed_commit_id": "cf9be5344a89dd8e14f8241ec69de9c984ec1e05", "lines_deleted": {"145": "        return objcls.from_crawler(crawler, *args, **kwargs)\n", "147": "        return objcls.from_settings(settings, *args, **kwargs)\n", "149": "        return objcls(*args, **kwargs)\n"}, "lines_added": {"137": "\n", "140": "    Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n", "141": "    extension has not been implemented correctly).\n", "148": "        instance = objcls.from_crawler(crawler, *args, **kwargs)\n", "149": "        method_name = 'from_crawler'\n", "151": "        instance = objcls.from_settings(settings, *args, **kwargs)\n", "152": "        method_name = 'from_settings'\n", "154": "        instance = objcls(*args, **kwargs)\n", "155": "        method_name = '__new__'\n", "156": "    if instance is None:\n", "157": "        raise TypeError(\"%s.%s returned None\" % (objcls.__qualname__, method_name))\n", "158": "    return instance\n"}}
{"project": "scrapy", "bug": 37, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/request/__init__.py", "buggy_commit_id": "1d5c270ce8caf954ce83c8db262e2a35707e0c5e", "fixed_commit_id": "f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a", "lines_deleted": {"68": "        if ':' not in self._url:\n"}, "lines_added": {"68": "        if ('://' not in self._url) and (not self._url.startswith('data:')):\n"}}
{"project": "scrapy", "bug": 38, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/request/form.py", "buggy_commit_id": "6cc6bbb5fc5c102271829a554772effb0444023c", "fixed_commit_id": "6c3970e6722191b642fd99c6c1bfed0d93010cab", "lines_deleted": {"172": "            'descendant::*[(self::input or self::button)'\n", "173": "            ' and re:test(@type, \"^submit$\", \"i\")]'\n", "174": "            '|descendant::button[not(@type)]',\n"}, "lines_added": {"172": "            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n", "173": "            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n"}}
{"project": "scrapy", "bug": 39, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/spiders/__init__.py", "buggy_commit_id": "692975acb40c6394424dfb728b1ffa46b3b3c55d", "fixed_commit_id": "a1e8a8525d2312842c7e1cca8ba6e4e1a83084b7", "lines_deleted": {"68": "        if self.make_requests_from_url is not Spider.make_requests_from_url:\n", "70": "                \"Spider.make_requests_from_url method is deprecated; \"\n", "71": "                \"it won't be called in future Scrapy releases. \"\n", "72": "                \"Please override start_requests method instead.\"\n"}, "lines_added": {"68": "        cls = self.__class__\n", "69": "        if cls.make_requests_from_url is not Spider.make_requests_from_url:\n", "71": "                \"Spider.make_requests_from_url method is deprecated; it \"\n", "72": "                \"won't be called in future Scrapy releases. Please \"\n", "73": "                \"override Spider.start_requests method instead (see %s.%s).\" % (\n", "74": "                    cls.__module__, cls.__name__\n", "75": "                ),\n"}}
{"project": "scrapy", "bug": 40, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/exporters.py", "buggy_commit_id": "7d24df37380cd5a5b7394cd2534e240bd2eff0ca", "fixed_commit_id": "f1d971a5c0cdfe0f4fe5619146cd6818324fc98e", "lines_deleted": {"275": "        if self.binary:\n", "276": "            return to_bytes(value, encoding=self.encoding)\n", "277": "        else:\n", "278": "            return to_unicode(value, encoding=self.encoding)\n"}, "lines_added": {"275": "        encode_func = to_bytes if self.binary else to_unicode\n", "276": "        if isinstance(value, (six.text_type, bytes)):\n", "277": "            return encode_func(value, encoding=self.encoding)\n", "278": "        return value\n"}}
{"project": "scrapy", "bug": 1, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/spidermiddlewares/offsite.py", "buggy_commit_id": "c57512fa669e6f6b1b766a7639206a380f0d10ce", "fixed_commit_id": "9d9dea0d69709ef0f7aef67ddba1bd7bda25d273", "lines_deleted": {"57": "            if url_pattern.match(domain):\n", "61": "        domains = [re.escape(d) for d in allowed_domains if d is not None]\n"}, "lines_added": {"56": "        domains = []\n", "58": "            if domain is None:\n", "59": "                continue\n", "60": "            elif url_pattern.match(domain):\n", "64": "            else:\n", "65": "                domains.append(re.escape(domain))\n"}}
{"project": "scrapy", "bug": 2, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/datatypes.py", "buggy_commit_id": "f02c3d1dcf3e4880388d19e961e7911be5dc54ff", "fixed_commit_id": "439a3e59b8e858441f8d97dbc32f398db392330d", "lines_deleted": {"317": "        while len(self) >= self.limit:\n", "318": "            self.popitem(last=False)\n"}, "lines_added": {"317": "        if self.limit:\n", "318": "            while len(self) >= self.limit:\n", "319": "                self.popitem(last=False)\n"}}
{"project": "scrapy", "bug": 4, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/contracts/__init__.py", "buggy_commit_id": "c8f3d07e86dd41074971b5423fb932c2eda6db1e", "fixed_commit_id": "16dad81715d3970149c0cf7a318e73a0d84be1ff", "lines_deleted": {"86": "            exc_info = failure.value, failure.type, failure.getTracebackObject()\n"}, "lines_added": {"86": "            exc_info = failure.type, failure.value, failure.getTracebackObject()\n"}}
{"project": "scrapy", "bug": 5, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/response/__init__.py", "buggy_commit_id": "426da0ed07637e7efbcfb0fe49546e187d5d7f67", "fixed_commit_id": "acd2b8d43b5ebec7ffd364b6f335427041a0b98d", "lines_deleted": {}, "lines_added": {"122": "        elif url is None:\n", "123": "            raise ValueError(\"url can't be None\")\n"}}
{"project": "scrapy", "bug": 6, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/pipelines/images.py", "buggy_commit_id": "8aa2e4f9976d31bbe3a0014b4f1f96ace1b87043", "fixed_commit_id": "25f609e2a3c27ca7d7d98dbfddb2c049735935bb", "lines_deleted": {}, "lines_added": {"134": "        elif image.mode == 'P':\n", "135": "            image = image.convert(\"RGBA\")\n", "136": "            background = Image.new('RGBA', image.size, (255, 255, 255))\n", "137": "            background.paste(image, image)\n", "138": "            image = background.convert('RGB')\n"}}
{"project": "scrapy", "bug": 11, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/gz.py", "buggy_commit_id": "241bd00e76df142a24699819f8496bdec8f5c83a", "fixed_commit_id": "9de6f1ca757b7f200d15e94840c9d431cf202276", "lines_deleted": {"45": "                    output += f.extrabuf\n"}, "lines_added": {"45": "                    output += f.extrabuf[-f.extrasize:]\n"}}
{"project": "scrapy", "bug": 12, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/selector/unified.py", "buggy_commit_id": "34e7dadf38ba1796094c0c76e92ea8d9837681cc", "fixed_commit_id": "2c9a38d1f54a12c33d7c9a19e021c840c4a32dee", "lines_deleted": {}, "lines_added": {"48": "        if not(response is None or text is None):\n", "49": "           raise ValueError('%s.__init__() received both response and text'\n", "50": "                            % self.__class__.__name__)\n", "51": "\n"}}
{"project": "scrapy", "bug": 13, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/pipelines/images.py", "buggy_commit_id": "fa78849e335994a1617ed63221a70940c21cca20", "fixed_commit_id": "414857a593ad5b82fa21d6344928f43f93dc9f14", "lines_deleted": {"44": "    EXPIRES = 0\n"}, "lines_added": {"44": "    EXPIRES = 90\n"}}
{"project": "scrapy", "bug": 15, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/url.py", "buggy_commit_id": "b7925e42202d79d2ba9d00b6aded3a451c92fe81", "fixed_commit_id": "1aec5200bc81493623f2a4e077b4e80e104e47d5", "lines_deleted": {"45": "        to_native_str(parts.netloc.encode('idna')),\n"}, "lines_added": {"43": "    # IDNA encoding can fail for too long labels (>63 characters)\n", "44": "    # or missing labels (e.g. http://.example.com)\n", "45": "    try:\n", "46": "        netloc = parts.netloc.encode('idna')\n", "47": "    except UnicodeError:\n", "48": "        netloc = parts.netloc\n", "49": "\n", "52": "        to_native_str(netloc),\n"}}
{"project": "scrapy", "bug": 17, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/response.py", "buggy_commit_id": "ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16", "fixed_commit_id": "65c7c05060fd2d1fc161d4904243d5e0b31e202b", "lines_deleted": {"49": "\n", "50": "    >>> response_status_message(200)\n", "51": "    '200 OK'\n", "52": "\n", "53": "    >>> response_status_message(404)\n", "54": "    '404 Not Found'\n", "56": "    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))\n"}, "lines_added": {"50": "    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), \"Unknown Status\")))\n"}}
{"project": "scrapy", "bug": 18, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/responsetypes.py", "buggy_commit_id": "41588397c04356f2b0c393b61ed68271a08d6ccd", "fixed_commit_id": "cabed6f183cfb2ab778c57be8c75802fec5e54d4", "lines_deleted": {"61": "            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n"}, "lines_added": {"61": "            filename = to_native_str(content_disposition,\n", "62": "                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n"}}
{"project": "scrapy", "bug": 19, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/cookies.py", "buggy_commit_id": "e328a9b9dfa4fbc79c59ed4f45f757e998301c31", "fixed_commit_id": "1f743996ff00a7b728d59b93d0967e1eb50072f0", "lines_deleted": {"139": "    # python3 uses request.unverifiable\n", "144": "    def get_origin_req_host(self):\n", "145": "        return urlparse_cached(self.request).hostname\n"}, "lines_added": {"139": "    def get_origin_req_host(self):\n", "140": "        return urlparse_cached(self.request).hostname\n", "141": "\n", "142": "    # python3 uses attributes instead of methods\n", "143": "    @property\n", "144": "    def full_url(self):\n", "145": "        return self.get_full_url()\n", "146": "\n", "147": "    @property\n", "148": "    def host(self):\n", "149": "        return self.get_host()\n", "150": "\n", "151": "    @property\n", "152": "    def type(self):\n", "153": "        return self.get_type()\n", "154": "\n", "155": "    @property\n", "160": "    def origin_req_host(self):\n", "161": "        return self.get_origin_req_host()\n"}}
{"project": "scrapy", "bug": 20, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/spiders/sitemap.py", "buggy_commit_id": "e328a9b9dfa4fbc79c59ed4f45f757e998301c31", "fixed_commit_id": "25c56159b86288311630cc0cf6db9d755aeeff1e", "lines_deleted": {"34": "            for url in sitemap_urls_from_robots(response.body):\n"}, "lines_added": {"34": "            for url in sitemap_urls_from_robots(response.text):\n"}}
{"project": "scrapy", "bug": 21, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/downloadermiddlewares/robotstxt.py", "buggy_commit_id": "43a53aca1207a82b663fe7a90c375546ce340a8e", "fixed_commit_id": "a8a6f050e71fbb7881076a8d6e2867e868d26016", "lines_deleted": {"103": "        self._parsers.pop(netloc).callback(None)\n"}, "lines_added": {"103": "        rp_dfd = self._parsers[netloc]\n", "104": "        self._parsers[netloc] = None\n", "105": "        rp_dfd.callback(None)\n"}}
{"project": "scrapy", "bug": 22, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/exporters.py", "buggy_commit_id": "a35aec71e96b0c0288c370afa425e8e700dca8b3", "fixed_commit_id": "bb2cf7c0d7199fffe0aa100e5c8a51c6b4b82fc2", "lines_deleted": {"146": "        else:\n"}, "lines_added": {"146": "        elif isinstance(serialized_value, six.text_type):\n", "148": "        else:\n", "149": "            self._xg_characters(str(serialized_value))\n"}}
{"project": "scrapy", "bug": 27, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/downloadermiddlewares/redirect.py", "buggy_commit_id": "280eab241680c93a763a3ef3a9ccd0c257259ca0", "fixed_commit_id": "d164398a27736f75286cc435eca69b06ff7c1c06", "lines_deleted": {"57": "               response.status in getattr(spider, 'handle_httpstatus_list', [])):\n"}, "lines_added": {"57": "               response.status in getattr(spider, 'handle_httpstatus_list', []) or\n", "58": "               response.status in request.meta.get('handle_httpstatus_list', []) or\n", "59": "               request.meta.get('handle_httpstatus_all', False)):\n"}}
{"project": "scrapy", "bug": 28, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/dupefilters.py", "buggy_commit_id": "e2f31f3018c0037f65982209c22f93b80a5d6e7b", "fixed_commit_id": "457b97c13ccf9a84f3dc7800c180cf059822c09a", "lines_deleted": {}, "lines_added": {"38": "            self.file.seek(0)\n"}}
{"project": "scrapy", "bug": 29, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/request.py", "buggy_commit_id": "5c4666a3d489bd3efa2e188de58721a125a5bfad", "fixed_commit_id": "8d45b3c4810cb5304ba1193b45697a0df1157326", "lines_deleted": {"81": "    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n"}, "lines_added": {"81": "    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n"}}
{"project": "scrapy", "bug": 32, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/crawler.py", "buggy_commit_id": "342cb622f1ea93268477da557099010bbd72529a", "fixed_commit_id": "aa6a72707daabfb6217f52e4774f2ff038f83dcc", "lines_deleted": {"211": "        configure_logging(settings)\n", "212": "        log_scrapy_info(settings)\n"}, "lines_added": {"211": "        configure_logging(self.settings)\n", "212": "        log_scrapy_info(self.settings)\n"}}
{"project": "scrapy", "bug": 35, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/crawler.py", "buggy_commit_id": "0a5bbbaed3e182d2151b6b34357667901ece353f", "fixed_commit_id": "c3d3a9491412d2a91b0927a05908593dcd329e4a", "lines_deleted": {"194": "    cls_path = settings.get('SPIDER_LOADER_CLASS',\n", "195": "                            settings.get('SPIDER_MANAGER_CLASS'))\n"}, "lines_added": {"194": "    cls_path = settings.get('SPIDER_MANAGER_CLASS',\n", "195": "                            settings.get('SPIDER_LOADER_CLASS'))\n"}}
{"project": "scrapy", "bug": 36, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/utils/misc.py", "buggy_commit_id": "cb8140a42a27ede87b0880372024f2f1804618b8", "fixed_commit_id": "cf9be5344a89dd8e14f8241ec69de9c984ec1e05", "lines_deleted": {"145": "        return objcls.from_crawler(crawler, *args, **kwargs)\n", "147": "        return objcls.from_settings(settings, *args, **kwargs)\n", "149": "        return objcls(*args, **kwargs)\n"}, "lines_added": {"137": "\n", "140": "    Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an\n", "141": "    extension has not been implemented correctly).\n", "148": "        instance = objcls.from_crawler(crawler, *args, **kwargs)\n", "149": "        method_name = 'from_crawler'\n", "151": "        instance = objcls.from_settings(settings, *args, **kwargs)\n", "152": "        method_name = 'from_settings'\n", "154": "        instance = objcls(*args, **kwargs)\n", "155": "        method_name = '__new__'\n", "156": "    if instance is None:\n", "157": "        raise TypeError(\"%s.%s returned None\" % (objcls.__qualname__, method_name))\n", "158": "    return instance\n"}}
{"project": "scrapy", "bug": 37, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/request/__init__.py", "buggy_commit_id": "1d5c270ce8caf954ce83c8db262e2a35707e0c5e", "fixed_commit_id": "f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a", "lines_deleted": {"68": "        if ':' not in self._url:\n"}, "lines_added": {"68": "        if ('://' not in self._url) and (not self._url.startswith('data:')):\n"}}
{"project": "scrapy", "bug": 38, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/http/request/form.py", "buggy_commit_id": "6cc6bbb5fc5c102271829a554772effb0444023c", "fixed_commit_id": "6c3970e6722191b642fd99c6c1bfed0d93010cab", "lines_deleted": {"172": "            'descendant::*[(self::input or self::button)'\n", "173": "            ' and re:test(@type, \"^submit$\", \"i\")]'\n", "174": "            '|descendant::button[not(@type)]',\n"}, "lines_added": {"172": "            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n", "173": "            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n"}}
{"project": "scrapy", "bug": 39, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/spiders/__init__.py", "buggy_commit_id": "692975acb40c6394424dfb728b1ffa46b3b3c55d", "fixed_commit_id": "a1e8a8525d2312842c7e1cca8ba6e4e1a83084b7", "lines_deleted": {"68": "        if self.make_requests_from_url is not Spider.make_requests_from_url:\n", "70": "                \"Spider.make_requests_from_url method is deprecated; \"\n", "71": "                \"it won't be called in future Scrapy releases. \"\n", "72": "                \"Please override start_requests method instead.\"\n"}, "lines_added": {"68": "        cls = self.__class__\n", "69": "        if cls.make_requests_from_url is not Spider.make_requests_from_url:\n", "71": "                \"Spider.make_requests_from_url method is deprecated; it \"\n", "72": "                \"won't be called in future Scrapy releases. Please \"\n", "73": "                \"override Spider.start_requests method instead (see %s.%s).\" % (\n", "74": "                    cls.__module__, cls.__name__\n", "75": "                ),\n"}}
{"project": "scrapy", "bug": 40, "project_url": "https://github.com/scrapy/scrapy", "file_changed": "scrapy/exporters.py", "buggy_commit_id": "7d24df37380cd5a5b7394cd2534e240bd2eff0ca", "fixed_commit_id": "f1d971a5c0cdfe0f4fe5619146cd6818324fc98e", "lines_deleted": {"275": "        if self.binary:\n", "276": "            return to_bytes(value, encoding=self.encoding)\n", "277": "        else:\n", "278": "            return to_unicode(value, encoding=self.encoding)\n"}, "lines_added": {"275": "        encode_func = to_bytes if self.binary else to_unicode\n", "276": "        if isinstance(value, (six.text_type, bytes)):\n", "277": "            return encode_func(value, encoding=self.encoding)\n", "278": "        return value\n"}}
