{"prefix": "        tmp_stdout, tmp_stderr = tempfile.TemporaryFile(), tempfile.TemporaryFile()\n        proc = subprocess.Popen(args, stdout=tmp_stdout, stderr=tmp_stderr,\n                                env=self.get_environment(), close_fds=True,\n                                universal_newlines=True)\n        try:\n            with SparkRunContext(proc):\n                proc.wait()\n            tmp_stdout.seek(0)\n            stdout = \"\".join(map(lambda s: s.decode('utf-8'), tmp_stdout.readlines()))\n            logger.info(\"Spark job stdout:\\n{0}\".format(stdout))\n            if proc.returncode != 0:\n                tmp_stderr.seek(0)\n                stderr = \"\".join(map(lambda s: s.decode('utf-8'), tmp_stderr.readlines()))\n                raise SparkJobError('Spark job failed {0}'.format(repr(args)), out=stdout, err=stderr)\n        finally:\n            tmp_stderr.close()\n            tmp_stdout.close()\n\n    def _list_config(self, config):\n        if config and isinstance(config, six.string_types):\n            return list(map(lambda x: x.strip(), config.split(',')))\n\n    def _dict_config(self, config):\n        if config and isinstance(config, six.string_types):\n            return dict(map(lambda i: i.split('='), config.split('|')))\n\n    def _text_arg(self, name, value):\n        if value:\n            return [name, value]\n        return []\n\n    def _list_arg(self, name, value):\n        if value and isinstance(value, (list, tuple)):\n            return [name, ','.join(value)]\n        return []\n\n    def _dict_arg(self, name, value):\n        command = []\n        if value and isinstance(value, dict):\n            for prop, value in value.items():\n", "suffix": "        return command\n\n    def _flag_arg(self, name, value):\n        if value:\n            return [name]\n        return []\n\n\nclass PySparkTask(SparkSubmitTask):\n    \"\"\"\n    Template task for running an inline PySpark job\n\n    Simply implement the ``main`` method in your subclass\n\n    You can optionally define package names to be distributed to the cluster\n    with ``py_packages`` (uses luigi's global py-packages configuration by default)\n\n    \"\"\"\n\n    # Path to the pyspark program passed to spark-submit\n    app = os.path.join(os.path.dirname(__file__), 'pyspark_runner.py')\n    # Python only supports the client deploy mode, force it\n    deploy_mode = \"client\"\n\n    @property\n    def name(self):\n        return self.__class__.__name__\n\n    @property\n    def py_packages(self):\n        packages = configuration.get_config().get('spark', 'py-packages', None)\n        if packages:\n            return map(lambda s: s.strip(), packages.split(','))\n\n    def setup(self, conf):\n        \"\"\"\n        Called by the pyspark_runner with a SparkConf instance that will be used to instantiate the SparkContext\n\n        :param conf: SparkConf\n        \"\"\"\n", "long_prefix": ["    def get_environment(self):\n        env = os.environ.copy()\n        hadoop_conf_dir = self.hadoop_conf_dir\n        if hadoop_conf_dir:\n            env['HADOOP_CONF_DIR'] = hadoop_conf_dir\n        return env\n\n    def spark_command(self):\n        command = [self.spark_submit]\n        command += self._text_arg('--master', self.master)\n        command += self._text_arg('--deploy-mode', self.deploy_mode)\n        command += self._text_arg('--name', self.name)\n        command += self._text_arg('--class', self.entry_class)\n        command += self._list_arg('--jars', self.jars)\n        command += self._list_arg('--py-files', self.py_files)\n        command += self._list_arg('--files', self.files)\n        command += self._list_arg('--archives', self.archives)\n        command += self._dict_arg('--conf', self.conf)\n        command += self._text_arg('--properties-file', self.properties_file)\n        command += self._text_arg('--driver-memory', self.driver_memory)\n        command += self._text_arg('--driver-java-options', self.driver_java_options)\n        command += self._text_arg('--driver-library-path', self.driver_library_path)\n        command += self._text_arg('--driver-class-path', self.driver_class_path)\n        command += self._text_arg('--executor-memory', self.executor_memory)\n        command += self._text_arg('--driver-cores', self.driver_cores)\n        command += self._flag_arg('--supervise', self.supervise)\n        command += self._text_arg('--total-executor-cores', self.total_executor_cores)\n        command += self._text_arg('--executor-cores', self.executor_cores)\n        command += self._text_arg('--queue', self.queue)\n        command += self._text_arg('--num-executors', self.num_executors)\n        return command\n\n    def app_command(self):\n        if not self.app:\n            raise NotImplementedError(\"subclass should define an app (.jar or .py file)\")\n        return [self.app] + self.app_options()\n\n    def run(self):\n        args = list(map(str, self.spark_command() + self.app_command()))\n        logger.info('Running: %s', repr(args))\n        tmp_stdout, tmp_stderr = tempfile.TemporaryFile(), tempfile.TemporaryFile()\n        proc = subprocess.Popen(args, stdout=tmp_stdout, stderr=tmp_stderr,\n                                env=self.get_environment(), close_fds=True,\n                                universal_newlines=True)\n        try:\n            with SparkRunContext(proc):\n                proc.wait()\n            tmp_stdout.seek(0)\n            stdout = \"\".join(map(lambda s: s.decode('utf-8'), tmp_stdout.readlines()))\n            logger.info(\"Spark job stdout:\\n{0}\".format(stdout))\n            if proc.returncode != 0:\n                tmp_stderr.seek(0)\n                stderr = \"\".join(map(lambda s: s.decode('utf-8'), tmp_stderr.readlines()))\n                raise SparkJobError('Spark job failed {0}'.format(repr(args)), out=stdout, err=stderr)\n        finally:\n            tmp_stderr.close()\n            tmp_stdout.close()\n\n    def _list_config(self, config):\n        if config and isinstance(config, six.string_types):\n            return list(map(lambda x: x.strip(), config.split(',')))\n\n    def _dict_config(self, config):\n        if config and isinstance(config, six.string_types):\n            return dict(map(lambda i: i.split('='), config.split('|')))\n\n    def _text_arg(self, name, value):\n        if value:\n            return [name, value]\n        return []\n\n    def _list_arg(self, name, value):\n        if value and isinstance(value, (list, tuple)):\n            return [name, ','.join(value)]\n        return []\n\n    def _dict_arg(self, name, value):\n        command = []\n        if value and isinstance(value, dict):\n            for prop, value in value.items():\n", "        return command\n\n    def _flag_arg(self, name, value):\n        if value:\n            return [name]\n        return []\n\n\nclass PySparkTask(SparkSubmitTask):\n    \"\"\"\n    Template task for running an inline PySpark job\n\n    Simply implement the ``main`` method in your subclass\n\n    You can optionally define package names to be distributed to the cluster\n    with ``py_packages`` (uses luigi's global py-packages configuration by default)\n\n    \"\"\"\n\n    # Path to the pyspark program passed to spark-submit\n    app = os.path.join(os.path.dirname(__file__), 'pyspark_runner.py')\n    # Python only supports the client deploy mode, force it\n    deploy_mode = \"client\"\n\n    @property\n    def name(self):\n        return self.__class__.__name__\n\n    @property\n    def py_packages(self):\n        packages = configuration.get_config().get('spark', 'py-packages', None)\n        if packages:\n            return map(lambda s: s.strip(), packages.split(','))\n\n    def setup(self, conf):\n        \"\"\"\n        Called by the pyspark_runner with a SparkConf instance that will be used to instantiate the SparkContext\n\n        :param conf: SparkConf\n        \"\"\"\n"]}