{"prefix": "        log.msg(format='Received %(signame)s twice, forcing unclean shutdown',\n                level=log.INFO, signame=signame)\n        self._stop_logging()\n        reactor.callFromThread(self._stop_reactor)\n\n    def start(self, stop_after_crawl=True):\n        if stop_after_crawl:\n            d = self.join()\n            # Don't start the reactor if the deferreds are already fired\n            if d.called:\n                return\n            d.addBoth(lambda _: self._stop_reactor())\n\n        cache_size = self.settings.getint('DNSCACHE_SIZE') if self.settings.getbool('DNSCACHE_ENABLED') else 0\n        reactor.installResolver(CachingThreadedResolver(reactor, cache_size,\n                                                            self.settings.getfloat('DNS_TIMEOUT')))\n        tp = reactor.getThreadPool()\n        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n        reactor.run(installSignalHandlers=False)  # blocking call\n\n    def _stop_logging(self):\n        if self.log_observer:\n            self.log_observer.stop()\n\n    def _stop_reactor(self, _=None):\n        try:\n            reactor.stop()\n        except RuntimeError:  # raised if already stopped or in shutdown stage\n            pass\n\n\ndef _get_spider_loader(settings):\n    \"\"\" Get SpiderLoader instance from settings \"\"\"\n    if settings.get('SPIDER_MANAGER_CLASS'):\n        warnings.warn(\n            'SPIDER_MANAGER_CLASS option is deprecated. '\n            'Please use SPIDER_LOADER_CLASS.',\n            category=ScrapyDeprecationWarning, stacklevel=2\n        )\n", "suffix": "    loader_cls = load_object(cls_path)\n    verifyClass(ISpiderLoader, loader_cls)\n    return loader_cls.from_settings(settings.frozencopy())\n", "long_prefix": ["    def _create_crawler(self, spidercls):\n        if isinstance(spidercls, six.string_types):\n            spidercls = self.spider_loader.load(spidercls)\n        return Crawler(spidercls, self.settings)\n\n    def _setup_crawler_logging(self, crawler):\n        log_observer = log.start_from_crawler(crawler)\n        if log_observer:\n            crawler.signals.connect(log_observer.stop, signals.engine_stopped)\n\n    def stop(self):\n        return defer.DeferredList([c.stop() for c in list(self.crawlers)])\n\n    @defer.inlineCallbacks\n    def join(self):\n        \"\"\"Wait for all managed crawlers to complete\"\"\"\n        while self._active:\n            yield defer.DeferredList(self._active)\n\n\nclass CrawlerProcess(CrawlerRunner):\n    \"\"\"A class to run multiple scrapy crawlers in a process simultaneously\"\"\"\n\n    def __init__(self, settings):\n        super(CrawlerProcess, self).__init__(settings)\n        install_shutdown_handlers(self._signal_shutdown)\n        self.stopping = False\n        self.log_observer = log.start_from_settings(self.settings)\n        log.scrapy_info(settings)\n\n    def _signal_shutdown(self, signum, _):\n        install_shutdown_handlers(self._signal_kill)\n        signame = signal_names[signum]\n        log.msg(format=\"Received %(signame)s, shutting down gracefully. Send again to force \",\n                level=log.INFO, signame=signame)\n        reactor.callFromThread(self.stop)\n\n    def _signal_kill(self, signum, _):\n        install_shutdown_handlers(signal.SIG_IGN)\n        signame = signal_names[signum]\n        log.msg(format='Received %(signame)s twice, forcing unclean shutdown',\n                level=log.INFO, signame=signame)\n        self._stop_logging()\n        reactor.callFromThread(self._stop_reactor)\n\n    def start(self, stop_after_crawl=True):\n        if stop_after_crawl:\n            d = self.join()\n            # Don't start the reactor if the deferreds are already fired\n            if d.called:\n                return\n            d.addBoth(lambda _: self._stop_reactor())\n\n        cache_size = self.settings.getint('DNSCACHE_SIZE') if self.settings.getbool('DNSCACHE_ENABLED') else 0\n        reactor.installResolver(CachingThreadedResolver(reactor, cache_size,\n                                                            self.settings.getfloat('DNS_TIMEOUT')))\n        tp = reactor.getThreadPool()\n        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n        reactor.run(installSignalHandlers=False)  # blocking call\n\n    def _stop_logging(self):\n        if self.log_observer:\n            self.log_observer.stop()\n\n    def _stop_reactor(self, _=None):\n        try:\n            reactor.stop()\n        except RuntimeError:  # raised if already stopped or in shutdown stage\n            pass\n\n\ndef _get_spider_loader(settings):\n    \"\"\" Get SpiderLoader instance from settings \"\"\"\n    if settings.get('SPIDER_MANAGER_CLASS'):\n        warnings.warn(\n            'SPIDER_MANAGER_CLASS option is deprecated. '\n            'Please use SPIDER_LOADER_CLASS.',\n            category=ScrapyDeprecationWarning, stacklevel=2\n        )\n", "    loader_cls = load_object(cls_path)\n    verifyClass(ISpiderLoader, loader_cls)\n    return loader_cls.from_settings(settings.frozencopy())\n"]}