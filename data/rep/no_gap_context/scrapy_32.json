{"prefix": "\n        Returns a deferred that is fired when they all have ended.\n        \"\"\"\n        return defer.DeferredList([c.stop() for c in list(self.crawlers)])\n\n    @defer.inlineCallbacks\n    def join(self):\n        \"\"\"\n        join()\n\n        Returns a deferred that is fired when all managed :attr:`crawlers` have\n        completed their executions.\n        \"\"\"\n        while self._active:\n            yield defer.DeferredList(self._active)\n\n\nclass CrawlerProcess(CrawlerRunner):\n    \"\"\"\n    A class to run multiple scrapy crawlers in a process simultaneously.\n\n    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n    for starting a Twisted `reactor`_ and handling shutdown signals, like the\n    keyboard interrupt command Ctrl-C. It also configures top-level logging.\n\n    This utility should be a better fit than\n    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n    Twisted `reactor`_ within your application.\n\n    The CrawlerProcess object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n    \"\"\"\n\n    def __init__(self, settings):\n        super(CrawlerProcess, self).__init__(settings)\n        install_shutdown_handlers(self._signal_shutdown)\n", "suffix": "\n    def _signal_shutdown(self, signum, _):\n        install_shutdown_handlers(self._signal_kill)\n        signame = signal_names[signum]\n        logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \",\n                    {'signame': signame})\n        reactor.callFromThread(self.stop)\n\n    def _signal_kill(self, signum, _):\n        install_shutdown_handlers(signal.SIG_IGN)\n        signame = signal_names[signum]\n        logger.info('Received %(signame)s twice, forcing unclean shutdown',\n                    {'signame': signame})\n        reactor.callFromThread(self._stop_reactor)\n\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        This method starts a Twisted `reactor`_, adjusts its pool size to\n        :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based\n        on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        If `stop_after_crawl` is True, the reactor will be stopped after all\n        crawlers have finished, using :meth:`join`.\n\n        :param boolean stop_after_crawl: stop or not the reactor when all\n            crawlers have finished\n        \"\"\"\n        if stop_after_crawl:\n            d = self.join()\n            # Don't start the reactor if the deferreds are already fired\n            if d.called:\n                return\n            d.addBoth(lambda _: self._stop_reactor())\n\n        cache_size = self.settings.getint('DNSCACHE_SIZE') if self.settings.getbool('DNSCACHE_ENABLED') else 0\n        reactor.installResolver(CachingThreadedResolver(reactor, cache_size,\n                                                            self.settings.getfloat('DNS_TIMEOUT')))\n        tp = reactor.getThreadPool()\n        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n", "long_prefix": ["        keeping track of it so it can be stopped later.\n\n        If `crawler_or_spidercls` isn't a :class:`~scrapy.crawler.Crawler`\n        instance, this method will try to create one using this parameter as\n        the spider class given to it.\n\n        Returns a deferred that is fired when the crawling is finished.\n\n        :param crawler_or_spidercls: already created crawler, or a spider class\n            or spider's name inside the project to create it\n        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n            :class:`~scrapy.spiders.Spider` subclass or string\n\n        :param list args: arguments to initialize the spider\n\n        :param dict kwargs: keyword arguments to initialize the spider\n        \"\"\"\n        crawler = crawler_or_spidercls\n        if not isinstance(crawler_or_spidercls, Crawler):\n            crawler = self._create_crawler(crawler_or_spidercls)\n\n        self.crawlers.add(crawler)\n        d = crawler.crawl(*args, **kwargs)\n        self._active.add(d)\n\n        def _done(result):\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            return result\n\n        return d.addBoth(_done)\n\n    def _create_crawler(self, spidercls):\n        if isinstance(spidercls, six.string_types):\n            spidercls = self.spider_loader.load(spidercls)\n        return Crawler(spidercls, self.settings)\n\n    def stop(self):\n        \"\"\"\n        Stops simultaneously all the crawling jobs taking place.\n\n        Returns a deferred that is fired when they all have ended.\n        \"\"\"\n        return defer.DeferredList([c.stop() for c in list(self.crawlers)])\n\n    @defer.inlineCallbacks\n    def join(self):\n        \"\"\"\n        join()\n\n        Returns a deferred that is fired when all managed :attr:`crawlers` have\n        completed their executions.\n        \"\"\"\n        while self._active:\n            yield defer.DeferredList(self._active)\n\n\nclass CrawlerProcess(CrawlerRunner):\n    \"\"\"\n    A class to run multiple scrapy crawlers in a process simultaneously.\n\n    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n    for starting a Twisted `reactor`_ and handling shutdown signals, like the\n    keyboard interrupt command Ctrl-C. It also configures top-level logging.\n\n    This utility should be a better fit than\n    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n    Twisted `reactor`_ within your application.\n\n    The CrawlerProcess object must be instantiated with a\n    :class:`~scrapy.settings.Settings` object.\n\n    This class shouldn't be needed (since Scrapy is responsible of using it\n    accordingly) unless writing scripts that manually handle the crawling\n    process. See :ref:`run-from-script` for an example.\n    \"\"\"\n\n    def __init__(self, settings):\n        super(CrawlerProcess, self).__init__(settings)\n        install_shutdown_handlers(self._signal_shutdown)\n", "\n    def _signal_shutdown(self, signum, _):\n        install_shutdown_handlers(self._signal_kill)\n        signame = signal_names[signum]\n        logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \",\n                    {'signame': signame})\n        reactor.callFromThread(self.stop)\n\n    def _signal_kill(self, signum, _):\n        install_shutdown_handlers(signal.SIG_IGN)\n        signame = signal_names[signum]\n        logger.info('Received %(signame)s twice, forcing unclean shutdown',\n                    {'signame': signame})\n        reactor.callFromThread(self._stop_reactor)\n\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        This method starts a Twisted `reactor`_, adjusts its pool size to\n        :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based\n        on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        If `stop_after_crawl` is True, the reactor will be stopped after all\n        crawlers have finished, using :meth:`join`.\n\n        :param boolean stop_after_crawl: stop or not the reactor when all\n            crawlers have finished\n        \"\"\"\n        if stop_after_crawl:\n            d = self.join()\n            # Don't start the reactor if the deferreds are already fired\n            if d.called:\n                return\n            d.addBoth(lambda _: self._stop_reactor())\n\n        cache_size = self.settings.getint('DNSCACHE_SIZE') if self.settings.getbool('DNSCACHE_ENABLED') else 0\n        reactor.installResolver(CachingThreadedResolver(reactor, cache_size,\n                                                            self.settings.getfloat('DNS_TIMEOUT')))\n        tp = reactor.getThreadPool()\n        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n"]}