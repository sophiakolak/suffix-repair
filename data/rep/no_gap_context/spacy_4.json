{"prefix": "        if len(sentences) % n_sents == 0:\n            doc = create_doc(sentences, i)\n            docs.append(doc)\n            sentences = []\n    if sentences:\n        doc = create_doc(sentences, i)\n        docs.append(doc)\n    return docs\n\n\ndef is_ner(tag):\n    \"\"\"\n    Check the 10th column of the first token to determine if the file contains\n    NER tags\n    \"\"\"\n    tag_match = re.match(\"([A-Z_]+)-([A-Z_]+)\", tag)\n    if tag_match:\n        return True\n    elif tag == \"O\":\n        return True\n    else:\n        return False\n\n\ndef read_conllx(input_data, use_morphology=False, n=0):\n    i = 0\n    for sent in input_data.strip().split(\"\\n\\n\"):\n        lines = sent.strip().split(\"\\n\")\n        if lines:\n            while lines[0].startswith(\"#\"):\n                lines.pop(0)\n            tokens = []\n            for line in lines:\n\n                parts = line.split(\"\\t\")\n                id_, word, lemma, pos, tag, morph, head, dep, _1, iob = parts\n                if \"-\" in id_ or \".\" in id_:\n                    continue\n                try:\n                    id_ = int(id_) - 1\n", "suffix": "                    dep = \"ROOT\" if dep == \"root\" else dep\n                    tag = pos if tag == \"_\" else tag\n                    tag = tag + \"__\" + morph if use_morphology else tag\n                    iob = iob if iob else \"O\"\n                    tokens.append((id_, word, tag, head, dep, iob))\n                except:  # noqa: E722\n                    print(line)\n                    raise\n            tuples = [list(t) for t in zip(*tokens)]\n            yield (None, [[tuples, []]])\n            i += 1\n            if n >= 1 and i >= n:\n                break\n\n\ndef simplify_tags(iob):\n    \"\"\"\n    Simplify tags obtained from the dataset in order to follow Wikipedia\n    scheme (PER, LOC, ORG, MISC). 'PER', 'LOC' and 'ORG' keep their tags, while\n    'GPE_LOC' is simplified to 'LOC', 'GPE_ORG' to 'ORG' and all remaining tags to\n    'MISC'.\n    \"\"\"\n    new_iob = []\n    for tag in iob:\n        tag_match = re.match(\"([A-Z_]+)-([A-Z_]+)\", tag)\n        if tag_match:\n            prefix = tag_match.group(1)\n            suffix = tag_match.group(2)\n            if suffix == \"GPE_LOC\":\n                suffix = \"LOC\"\n            elif suffix == \"GPE_ORG\":\n                suffix = \"ORG\"\n            elif suffix != \"PER\" and suffix != \"LOC\" and suffix != \"ORG\":\n                suffix = \"MISC\"\n            tag = prefix + \"-\" + suffix\n        new_iob.append(tag)\n    return new_iob\n\n\ndef generate_sentence(sent, has_ner_tags):\n", "long_prefix": ["# coding: utf8\nfrom __future__ import unicode_literals\n\nimport re\n\nfrom ...gold import iob_to_biluo\n\n\ndef conllu2json(input_data, n_sents=10, use_morphology=False, lang=None, **_):\n    \"\"\"\n    Convert conllu files into JSON format for use with train cli.\n    use_morphology parameter enables appending morphology to tags, which is\n    useful for languages such as Spanish, where UD tags are not so rich.\n\n    Extract NER tags if available and convert them so that they follow\n    BILUO and the Wikipedia scheme\n    \"\"\"\n    # by @dvsrepo, via #11 explosion/spacy-dev-resources\n    # by @katarkor\n    docs = []\n    sentences = []\n    conll_tuples = read_conllx(input_data, use_morphology=use_morphology)\n    checked_for_ner = False\n    has_ner_tags = False\n    for i, (raw_text, tokens) in enumerate(conll_tuples):\n        sentence, brackets = tokens[0]\n        if not checked_for_ner:\n            has_ner_tags = is_ner(sentence[5][0])\n            checked_for_ner = True\n        sentences.append(generate_sentence(sentence, has_ner_tags))\n        # Real-sized documents could be extracted using the comments on the\n        # conluu document\n        if len(sentences) % n_sents == 0:\n            doc = create_doc(sentences, i)\n            docs.append(doc)\n            sentences = []\n    if sentences:\n        doc = create_doc(sentences, i)\n        docs.append(doc)\n    return docs\n\n\ndef is_ner(tag):\n    \"\"\"\n    Check the 10th column of the first token to determine if the file contains\n    NER tags\n    \"\"\"\n    tag_match = re.match(\"([A-Z_]+)-([A-Z_]+)\", tag)\n    if tag_match:\n        return True\n    elif tag == \"O\":\n        return True\n    else:\n        return False\n\n\ndef read_conllx(input_data, use_morphology=False, n=0):\n    i = 0\n    for sent in input_data.strip().split(\"\\n\\n\"):\n        lines = sent.strip().split(\"\\n\")\n        if lines:\n            while lines[0].startswith(\"#\"):\n                lines.pop(0)\n            tokens = []\n            for line in lines:\n\n                parts = line.split(\"\\t\")\n                id_, word, lemma, pos, tag, morph, head, dep, _1, iob = parts\n                if \"-\" in id_ or \".\" in id_:\n                    continue\n                try:\n                    id_ = int(id_) - 1\n", "                    dep = \"ROOT\" if dep == \"root\" else dep\n                    tag = pos if tag == \"_\" else tag\n                    tag = tag + \"__\" + morph if use_morphology else tag\n                    iob = iob if iob else \"O\"\n                    tokens.append((id_, word, tag, head, dep, iob))\n                except:  # noqa: E722\n                    print(line)\n                    raise\n            tuples = [list(t) for t in zip(*tokens)]\n            yield (None, [[tuples, []]])\n            i += 1\n            if n >= 1 and i >= n:\n                break\n\n\ndef simplify_tags(iob):\n    \"\"\"\n    Simplify tags obtained from the dataset in order to follow Wikipedia\n    scheme (PER, LOC, ORG, MISC). 'PER', 'LOC' and 'ORG' keep their tags, while\n    'GPE_LOC' is simplified to 'LOC', 'GPE_ORG' to 'ORG' and all remaining tags to\n    'MISC'.\n    \"\"\"\n    new_iob = []\n    for tag in iob:\n        tag_match = re.match(\"([A-Z_]+)-([A-Z_]+)\", tag)\n        if tag_match:\n            prefix = tag_match.group(1)\n            suffix = tag_match.group(2)\n            if suffix == \"GPE_LOC\":\n                suffix = \"LOC\"\n            elif suffix == \"GPE_ORG\":\n                suffix = \"ORG\"\n            elif suffix != \"PER\" and suffix != \"LOC\" and suffix != \"ORG\":\n                suffix = \"MISC\"\n            tag = prefix + \"-\" + suffix\n        new_iob.append(tag)\n    return new_iob\n\n\ndef generate_sentence(sent, has_ner_tags):\n"]}