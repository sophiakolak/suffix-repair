{"prefix": "    from io import BytesIO\nfrom gzip import GzipFile\n\nimport six\nimport re\n\n# - Python>=3.5 GzipFile's read() has issues returning leftover\n#   uncompressed data when input is corrupted\n#   (regression or bug-fix compared to Python 3.4)\n# - read1(), which fetches data before raising EOFError on next call\n#   works here but is only available from Python>=3.3\n# - scrapy does not support Python 3.2\n# - Python 2.7 GzipFile works fine with standard read() + extrabuf\nif six.PY2:\n    def read1(gzf, size=-1):\n        return gzf.read(size)\nelse:\n    def read1(gzf, size=-1):\n        return gzf.read1(size)\n\n\ndef gunzip(data):\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output = b''\n    chunk = b'.'\n    while chunk:\n        try:\n            chunk = read1(f, 8196)\n            output += chunk\n        except (IOError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output is '' and f.extrabuf\n            # contains the whole page content\n            if output or getattr(f, 'extrabuf', None):\n                try:\n", "suffix": "                finally:\n                    break\n            else:\n                raise\n    return output\n\n_is_gzipped = re.compile(br'^application/(x-)?gzip\\b', re.I).search\n_is_octetstream = re.compile(br'^(application|binary)/octet-stream\\b', re.I).search\n\ndef is_gzipped(response):\n    \"\"\"Return True if the response is gzipped, or False otherwise\"\"\"\n    ctype = response.headers.get('Content-Type', b'')\n    cenc = response.headers.get('Content-Encoding', b'').lower()\n    return (_is_gzipped(ctype) or\n            (_is_octetstream(ctype) and cenc in (b'gzip', b'x-gzip')))\n", "long_prefix": ["import struct\n\ntry:\n    from cStringIO import StringIO as BytesIO\nexcept ImportError:\n    from io import BytesIO\nfrom gzip import GzipFile\n\nimport six\nimport re\n\n# - Python>=3.5 GzipFile's read() has issues returning leftover\n#   uncompressed data when input is corrupted\n#   (regression or bug-fix compared to Python 3.4)\n# - read1(), which fetches data before raising EOFError on next call\n#   works here but is only available from Python>=3.3\n# - scrapy does not support Python 3.2\n# - Python 2.7 GzipFile works fine with standard read() + extrabuf\nif six.PY2:\n    def read1(gzf, size=-1):\n        return gzf.read(size)\nelse:\n    def read1(gzf, size=-1):\n        return gzf.read1(size)\n\n\ndef gunzip(data):\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output = b''\n    chunk = b'.'\n    while chunk:\n        try:\n            chunk = read1(f, 8196)\n            output += chunk\n        except (IOError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output is '' and f.extrabuf\n            # contains the whole page content\n            if output or getattr(f, 'extrabuf', None):\n                try:\n", "                finally:\n                    break\n            else:\n                raise\n    return output\n\n_is_gzipped = re.compile(br'^application/(x-)?gzip\\b', re.I).search\n_is_octetstream = re.compile(br'^(application|binary)/octet-stream\\b', re.I).search\n\ndef is_gzipped(response):\n    \"\"\"Return True if the response is gzipped, or False otherwise\"\"\"\n    ctype = response.headers.get('Content-Type', b'')\n    cenc = response.headers.get('Content-Encoding', b'').lower()\n    return (_is_gzipped(ctype) or\n            (_is_octetstream(ctype) and cenc in (b'gzip', b'x-gzip')))\n"]}