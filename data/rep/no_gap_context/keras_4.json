{"prefix": "            m_t_prime = m_t / (1. - m_schedule_next)\n            v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n            v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n            m_t_bar = (1. - momentum_cache_t) * g_prime + (\n                momentum_cache_t_1 * m_t_prime)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n\n            p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'epsilon': self.epsilon,\n                  'schedule_decay': self.schedule_decay}\n        base_config = super(Nadam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass TFOptimizer(Optimizer):\n    \"\"\"Wrapper class for native TensorFlow optimizers.\n    \"\"\"\n\n    def __init__(self, optimizer):\n        self.optimizer = optimizer\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n", "suffix": "        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(\n            grads, global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    @property\n    def weights(self):\n        raise NotImplementedError\n\n    def get_config(self):\n        raise NotImplementedError\n\n    def from_config(self, config):\n        raise NotImplementedError\n\n\n# Aliases.\n\nsgd = SGD\nrmsprop = RMSprop\nadagrad = Adagrad\nadadelta = Adadelta\nadam = Adam\nadamax = Adamax\nnadam = Nadam\n\n\ndef serialize(optimizer):\n    return serialize_keras_object(optimizer)\n\n\ndef deserialize(config, custom_objects=None):\n    \"\"\"Inverse of the `serialize` function.\n\n    # Arguments\n        config: Optimizer configuration dictionary.\n        custom_objects: Optional dictionary mapping\n            names (strings) to custom objects\n            (classes and functions)\n", "long_prefix": ["    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, schedule_decay=0.004, **kwargs):\n        super(Nadam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.m_schedule = K.variable(1., name='m_schedule')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.schedule_decay = schedule_decay\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        # Due to the recommendations in [2], i.e. warming momentum schedule\n        momentum_cache_t = self.beta_1 * (1. - 0.5 * (\n            K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (\n            K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n        m_schedule_new = self.m_schedule * momentum_cache_t\n        m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n        self.updates.append((self.m_schedule, m_schedule_new))\n\n        shapes = [K.int_shape(p) for p in params]\n        ms = [K.zeros(shape) for shape in shapes]\n        vs = [K.zeros(shape) for shape in shapes]\n\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            # the following equations given in [1]\n            g_prime = g / (1. - m_schedule_new)\n            m_t = self.beta_1 * m + (1. - self.beta_1) * g\n            m_t_prime = m_t / (1. - m_schedule_next)\n            v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n            v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n            m_t_bar = (1. - momentum_cache_t) * g_prime + (\n                momentum_cache_t_1 * m_t_prime)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n\n            p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'epsilon': self.epsilon,\n                  'schedule_decay': self.schedule_decay}\n        base_config = super(Nadam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass TFOptimizer(Optimizer):\n    \"\"\"Wrapper class for native TensorFlow optimizers.\n    \"\"\"\n\n    def __init__(self, optimizer):\n        self.optimizer = optimizer\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n", "        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(\n            grads, global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    @property\n    def weights(self):\n        raise NotImplementedError\n\n    def get_config(self):\n        raise NotImplementedError\n\n    def from_config(self, config):\n        raise NotImplementedError\n\n\n# Aliases.\n\nsgd = SGD\nrmsprop = RMSprop\nadagrad = Adagrad\nadadelta = Adadelta\nadam = Adam\nadamax = Adamax\nnadam = Nadam\n\n\ndef serialize(optimizer):\n    return serialize_keras_object(optimizer)\n\n\ndef deserialize(config, custom_objects=None):\n    \"\"\"Inverse of the `serialize` function.\n\n    # Arguments\n        config: Optimizer configuration dictionary.\n        custom_objects: Optional dictionary mapping\n            names (strings) to custom objects\n            (classes and functions)\n"]}