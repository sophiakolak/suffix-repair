{"prefix": "    def range_less_than(_, current_input):\n        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(\n            max_num_labels_tns, current_input)\n\n    init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)\n    dense_mask = functional_ops.scan(range_less_than, label_lengths,\n                                     initializer=init, parallel_iterations=1)\n    dense_mask = dense_mask[:, 0, :]\n\n    label_array = tf.reshape(tf.tile(tf.range(label_shape[1]), num_batches_tns),\n                             label_shape)\n    label_ind = tf.boolean_mask(label_array, dense_mask)\n\n    batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(label_shape[0]),\n                                                  max_num_labels_tns), reverse(label_shape, 0)))\n    batch_ind = tf.boolean_mask(batch_array, dense_mask)\n    indices = tf.transpose(tf.reshape(concatenate([batch_ind, label_ind], axis=0), [2, -1]))\n\n    vals_sparse = tf.gather_nd(labels, indices)\n\n    return tf.SparseTensor(tf.to_int64(indices), vals_sparse, tf.to_int64(label_shape))\n\n\ndef ctc_batch_cost(y_true, y_pred, input_length, label_length):\n    \"\"\"Runs CTC loss algorithm on each batch element.\n\n    # Arguments\n        y_true: tensor `(samples, max_string_length)`\n            containing the truth labels.\n        y_pred: tensor `(samples, time_steps, num_categories)`\n            containing the prediction, or output of the softmax.\n        input_length: tensor `(samples, 1)` containing the sequence length for\n            each batch item in `y_pred`.\n        label_length: tensor `(samples, 1)` containing the sequence length for\n            each batch item in `y_true`.\n\n    # Returns\n        Tensor with shape (samples,1) containing the\n            CTC loss of each element.\n    \"\"\"\n", "suffix": "    sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n\n    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n\n    return tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\n                                       labels=sparse_labels,\n                                       sequence_length=input_length), 1)\n\n\ndef ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n               top_paths=1):\n    \"\"\"Decodes the output of a softmax.\n\n    Can use either greedy search (also known as best path)\n    or a constrained dictionary search.\n\n    # Arguments\n        y_pred: tensor `(samples, time_steps, num_categories)`\n            containing the prediction, or output of the softmax.\n        input_length: tensor `(samples, )` containing the sequence length for\n            each batch item in `y_pred`.\n        greedy: perform much faster best-path search if `true`.\n            This does not use a dictionary.\n        beam_width: if `greedy` is `false`: a beam search decoder will be used\n            with a beam of this width.\n        top_paths: if `greedy` is `false`,\n            how many of the most probable paths will be returned.\n\n    # Returns\n        Tuple:\n            List: if `greedy` is `true`, returns a list of one element that\n                contains the decoded sequence.\n                If `false`, returns the `top_paths` most probable\n                decoded sequences.\n                Important: blank labels are returned as `-1`.\n            Tensor `(top_paths, )` that contains\n                the log probability of each decoded sequence.\n    \"\"\"\n    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n    input_length = tf.to_int32(input_length)\n", "long_prefix": ["    two standard deviations from the mean are dropped and re-picked.\n\n    # Arguments\n        shape: A tuple of integers, the shape of tensor to create.\n        mean: Mean of the values.\n        stddev: Standard deviation of the values.\n        dtype: String, dtype of returned tensor.\n        seed: Integer, random seed.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(10e6)\n    return tf.truncated_normal(shape, mean, stddev, dtype=dtype, seed=seed)\n\n\n# CTC\n# TensorFlow has a native implementation, but it uses sparse tensors\n# and therefore requires a wrapper for Keras. The functions below convert\n# dense to sparse tensors and also wraps up the beam search code that is\n# in TensorFlow's CTC implementation\n\n\ndef ctc_label_dense_to_sparse(labels, label_lengths):\n    \"\"\"Converts CTC labels from dense to sparse.\n\n    # Arguments\n        labels: dense CTC labels.\n        label_lengths: length of the labels.\n\n    # Returns\n        A sparse tensor representation of the labels.\n    \"\"\"\n    label_shape = tf.shape(labels)\n    num_batches_tns = tf.stack([label_shape[0]])\n    max_num_labels_tns = tf.stack([label_shape[1]])\n\n    def range_less_than(_, current_input):\n        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(\n            max_num_labels_tns, current_input)\n\n    init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)\n    dense_mask = functional_ops.scan(range_less_than, label_lengths,\n                                     initializer=init, parallel_iterations=1)\n    dense_mask = dense_mask[:, 0, :]\n\n    label_array = tf.reshape(tf.tile(tf.range(label_shape[1]), num_batches_tns),\n                             label_shape)\n    label_ind = tf.boolean_mask(label_array, dense_mask)\n\n    batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(label_shape[0]),\n                                                  max_num_labels_tns), reverse(label_shape, 0)))\n    batch_ind = tf.boolean_mask(batch_array, dense_mask)\n    indices = tf.transpose(tf.reshape(concatenate([batch_ind, label_ind], axis=0), [2, -1]))\n\n    vals_sparse = tf.gather_nd(labels, indices)\n\n    return tf.SparseTensor(tf.to_int64(indices), vals_sparse, tf.to_int64(label_shape))\n\n\ndef ctc_batch_cost(y_true, y_pred, input_length, label_length):\n    \"\"\"Runs CTC loss algorithm on each batch element.\n\n    # Arguments\n        y_true: tensor `(samples, max_string_length)`\n            containing the truth labels.\n        y_pred: tensor `(samples, time_steps, num_categories)`\n            containing the prediction, or output of the softmax.\n        input_length: tensor `(samples, 1)` containing the sequence length for\n            each batch item in `y_pred`.\n        label_length: tensor `(samples, 1)` containing the sequence length for\n            each batch item in `y_true`.\n\n    # Returns\n        Tensor with shape (samples,1) containing the\n            CTC loss of each element.\n    \"\"\"\n", "    sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n\n    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n\n    return tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\n                                       labels=sparse_labels,\n                                       sequence_length=input_length), 1)\n\n\ndef ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n               top_paths=1):\n    \"\"\"Decodes the output of a softmax.\n\n    Can use either greedy search (also known as best path)\n    or a constrained dictionary search.\n\n    # Arguments\n        y_pred: tensor `(samples, time_steps, num_categories)`\n            containing the prediction, or output of the softmax.\n        input_length: tensor `(samples, )` containing the sequence length for\n            each batch item in `y_pred`.\n        greedy: perform much faster best-path search if `true`.\n            This does not use a dictionary.\n        beam_width: if `greedy` is `false`: a beam search decoder will be used\n            with a beam of this width.\n        top_paths: if `greedy` is `false`,\n            how many of the most probable paths will be returned.\n\n    # Returns\n        Tuple:\n            List: if `greedy` is `true`, returns a list of one element that\n                contains the decoded sequence.\n                If `false`, returns the `top_paths` most probable\n                decoded sequences.\n                Important: blank labels are returned as `-1`.\n            Tensor `(top_paths, )` that contains\n                the log probability of each decoded sequence.\n    \"\"\"\n    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n    input_length = tf.to_int32(input_length)\n"]}