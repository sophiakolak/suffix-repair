{"candidate": "\n    def _stop_reactor(self):\n        reactor.stop() # ", "entropy": 18.570048093686296}
{"candidate": "        log.start(logfile=settings.get('LOG_FILE'), loglevel=settings.get('LOG_LEVEL'), logstdout=settings.getbool('LOG_STDOUT'))\n\n    def _stop_reactor(self):\n        if REACTOR.running:\n            REACTOR.stop()\n        logger.info(\"Scrapy shut down gracefully. Goodbye!\")\n", "entropy": 35.3912490496376}
{"candidate": "", "entropy": 9.317101072729999}
{"candidate": "", "entropy": 8.083112562999998}
{"candidate": "\n    def _stop_reactor(self):\n        if reactor.running:\n            reactor.stop()\n", "entropy": 12.994056180872303}
{"candidate": "        self._singleton = None\n\n    def _stop_reactor(self):\n        if reactor.running:\n            reactor.stop()\n", "entropy": 25.601067568066007}
{"candidate": "        configure_logging(settings, install_root_handler=False)\n\n    def _stop_reactor(self):\n        reactor.stop()\n", "entropy": 16.55670424189882}
{"candidate": "        self._reactor_stopped = False\n        self._stopping = False\n\n    def _stop_reactor(self):\n        if not self._stopping:\n            self._stopping = True\n            d = self.stop()\n            d.addBoth(lambda _: reactor.stop())\n            self._reactor_stopped = True\n", "entropy": 42.718326713317204}
{"candidate": "        configure_logging(settings, install_root_handler=False)\n", "entropy": 8.875504694441322}
{"candidate": "\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        Starts crawling and return a deferred that is fired when all crawling\n        jobs are finished.\n\n        This method is a helper that calls\n        :meth:`~scrapy.crawler.CrawlerRunner.start` with the arguments\n        ``stop_after_crawl=True`` and ``reactor=reactor``.\n\n        :param boolean stop_", "entropy": 47.10636398737071}
{"candidate": "        configure_logging(settings)\n\n    def start(self):\n        \"\"\"\n        This method starts a Twisted `reactor`_ and installs a DNS cache based\n        on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        It also adjust the reactor's threadpool based on\n        :setting:`REACTOR_THREADPOOL_MAXSIZE` setting.\n        \"\"\"", "entropy": 27.601797166094908}
{"candidate": "        configure_logging(settings)\n        self._reactor = None\n\n    @property\n    def reactor(self):\n        \"\"\"\n        Backwards-compatible accessor for the reactor.\n        \"\"\"\n        return self._reactor\n\n    def start(self, stop_after_crawl=True):\n        super(CrawlerProcess, self).start(stop_after_crawl)\n        self._reactor = reactor\n        self._", "entropy": 37.25758145470189}
{"candidate": "\n    def _stop_reactor(self):\n        reactor.stop()\n\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        Starts a Twisted `reactor`_, adjusts its pool size to\n        :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based\n        on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCAC", "entropy": 11.9765760081295}
{"candidate": "\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        This method starts a Twisted `reactor`_, adjusts its pool size to\n        :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based\n        on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        If `stop_after_crawl`", "entropy": 4.2408249268932705}
{"candidate": "\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        This method starts a Twisted `reactor`_, adjusts its pool size to\n        :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based\n        on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n\n        If `stop_after_crawl`", "entropy": 4.267685959724499}
{"candidate": "        self.configure_logging()\n\n    def configure_logging(self):\n        configure_logging(self.settings.get('LOG_ENABLED'),\n                          self.settings.getbool('LOG_ENCODING'),\n                          self.settings.get('LOG_FORMATTER'),\n                          self.settings.get('LOG_FILE'),\n                          self.settings.getint('LOG_LEVEL'),\n                ", "entropy": 17.528345645062046}
{"candidate": "        global _crawler_process\n        _crawler_process = self\n\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        This method starts a Twisted `reactor`_, adjusts its pool size to\n        :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based\n        on :setting:`DNSCACHE_ENABLED` and :setting:`DN", "entropy": 11.240754322067199}
{"candidate": "        configure_logging(self.settings, install_root_handler=False)\n        if self.settings.getbool('LOG_ENABLED'):\n            self.settings.set('LOG_FILE', sys.stdout, priority='cmdline')\n\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        This method starts a Twisted `reactor`_, adjusts its pool size to\n        :setting:", "entropy": 22.041564940711595}
{"candidate": "        self.log_observer = ScrapyFileLogObserver(settings.get('LOG_FILE'), level=logging.INFO)\n        global _logging_configured\n        _logging_configured = True\n\n    def start(self):\n        \"\"\"\n        Starts the crawling process with both the ``start`` and ``init`` signals\n        sent (see :ref:`topics-signals`).\n        \"\"\"\n        if", "entropy": 51.626138126350014}
{"candidate": "        install_thread_excepthook()\n\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        Starts the crawling process with the given settings.\n\n        This will initialize all crawlers found in the given settings by\n        instantiating them and calling their :meth:`~scrapy.crawler.Crawler.start()`\n        method (which in turn calls :meth:`~scrapy", "entropy": 49.2941074434894}
