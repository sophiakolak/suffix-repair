{"candidate": "\n    def _signal_shutdown(self, signum, _):\n        install_shutdown_handlers(self._signal_kill)\n        reactor.callFromThread(self.stop)\n\n    def _signal_kill(self, signum, _):\n        reactor.callFromThread(self.stop)\n        reactor.callFromThread(reactor.crash)\n\n    @defer.inlineCallbacks\n    def", "entropy": 14.371332860409996}
{"candidate": "        logging.config.dictConfig(settings.get('LOG_CONFIG'))\n\n    def start(self):\n        from twisted.internet import reactor\n        reactor.suggestThreadPoolSize(self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n        reactor.callWhenRunning(self._start_reactor, reactor)\n        reactor.run()\n\n    def _start_reactor(self, reactor):\n", "entropy": 23.5889161489073}
{"candidate": "        self._reactor = None\n\n    def _signal_shutdown(self, signum, _):\n        logger.info(\"Received %(signal)s, shutting down gracefully. Send again to force \",\n                    {'signal': Signals(signum).name})\n        reactor.callFromThread(self.stop)\n\n    def start(self):\n        \"\"\"\n        Starts the execution engine and blocks until it is finished.", "entropy": 25.155154856576694}
{"candidate": "        configure_logging(settings, install_root_handler=False)\n        self._reactor = None\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler.settings)\n\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        Starts a Twisted `reactor`_, adjusts its pool size, installs a SIGTERM\n        handler, instant", "entropy": 28.2531863033541}
{"candidate": "\n        if not settings.getbool('LOG_ENABLED'):\n            # Disable logging if it is not explicitly enabled to avoid\n            # configuring logging.\n            disable_logging()\n\n        self._stopping = False\n\n    def _signal_shutdown(self, signum, _):\n        install_shutdown_handlers(self._signal_kill)\n        self.stop()\n\n    def _signal_kill(", "entropy": 32.694599621687}
{"candidate": "\n    @defer.inlineCallbacks\n    def _install_signal_handlers(self):\n        signal.signal(signal.SIGTERM, lambda *args: reactor.callFromThread(\n            self._signal_shutdown, signal.SIGTERM, *args))\n        yield defer.succeed(None)\n\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n       ", "entropy": 32.9132275576002}
{"candidate": "\n    def start(self):\n        \"\"\"\n        Starts the twisted reactor and runs until shutdown.\n        \"\"\"\n        log.start(self.settings, logstdout=False)\n        reactor.run()\n\n    def stop(self):\n        \"\"\"\n        Stops the twisted reactor.\n        \"\"\"\n        if reactor.running:\n            reactor.stop()\n\n    def _signal_shutdown(self, signum, _):\n        install", "entropy": 33.898916182000015}
{"candidate": "\n    @defer.inlineCallbacks\n    def crawl(self, crawler_or_spidercls, *args, **kwargs):\n        \"\"\"\n        Shortcut method to create a :class:`~scrapy.crawler.Crawler` object\n        (if a class is given) and schedule it for execution, returning a\n        deferred that is fired when it finishes.\n\n        This method is a shortcut to :meth:`", "entropy": 35.11908048782799}
{"candidate": "        self.configure_logging()\n\n    def _signal_shutdown(self, signum, _):\n        self.stop()\n        reactor.callLater(0.1, reactor.stop)\n\n    def configure_logging(self):\n        \"\"\"Configure logging for the process\"\"\"\n        logging.config.dictConfig(self.settings.get_dict('LOG_CONFIG'))", "entropy": 33.668828490202976}
{"candidate": "        update_settings_log_path(settings)\n        if settings.getbool('LOG_ENABLED'):\n            configure_logging()\n\n    def _signal_shutdown(self, signum, _):\n        install_shutdown_handlers(self._signal_kill)\n        reactor.callFromThread(self.stop)\n\n    def _signal_kill(self, signum, _):\n        log.", "entropy": 38.133862505791}
{"candidate": "        logging.config.dictConfig(settings.get('LOG_CONFIG'))\n        self.spider_loader = _get_spider_loader(settings)\n\n    @defer.inlineCallbacks\n    def start(self):\n        \"\"\"\n        Starts a Twisted `reactor`_, adjusts its pool according to settings\n        and starts all crawlers taking into account Scrapy settings.\n\n        This method returns a deferred that is fired when", "entropy": 39.930965032063995}
{"candidate": "        self._reactor = reactor\n\n    def start(self):\n        \"\"\"\n        Start the crawling process. This method blocks until all crawling jobs\n        are finished.\n        \"\"\"\n        self._reactor.run()\n\n    def stop(self):\n        \"\"\"\n        Gracefully stop the crawling process. It returns a deferred that is\n        fired when the crawling job has finished.\n        \"\"\"\n        d = self._join()\n        self._re", "entropy": 41.110141607398695}
{"candidate": "\n        self.logger = logging.getLogger(__name__)\n\n        #: :type: twisted.internet.reactor.Reactor\n        self.reactor = reactor\n\n    def _signal_shutdown(self):\n        self.stop()\n\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        Starts the Twisted reactor. This method blocks until the reactor stops\n        running.\n\n        Parameters\n", "entropy": 42.90419037159038}
{"candidate": "        configure_logging(settings, install_root_handler=False)\n        enable_log_counter()\n\n    def _signal_shutdown(self, signum, _):\n        logger.info(\"Received sigint signal, shutting down gracefully. Send again to force \"\n                    \"shutdown.\")\n        install_shutdown_handlers(self._signal_kill)\n        reactor.callLater(3, self._kill,", "entropy": 42.66554034181479}
{"candidate": "\n    def start(self, stop_after_crawl=True):\n        \"\"\"Run all crawlers.\n\n        This method must be called after\n        :meth:`~scrapy.crawler.CrawlerProcess.__init__`, as it needs to\n        configure the logging system.\n\n        Parameters\n        ----------\n        stop_after_crawl: bool\n            If ``True``, it will stop the reactor after all crawlers have", "entropy": 44.99654328649389}
{"candidate": "\n    def crawl(self, crawler_or_spidercls, *args, **kwargs):\n        \"\"\"Make a crawler crawl by retrieving its spider and then calling\n        :meth:`~crawlers.Crawler.crawl`\n        \"\"\"\n        crawler = self._create_crawler(crawler_or_spidercls)\n        self.crawlers.add(crawler)\n        dfd =", "entropy": 46.22583399711904}
{"candidate": "        setup_logging(settings, install_root_handler=False)\n        self.start_reactor()\n\n    def start_reactor(self):\n        \"\"\"\n        Install the reactor used to run the crawlers.\n        \"\"\"\n        if is_asyncio_reactor_installed():\n            self._reactor = None\n        else:\n            self._reactor = reactor\n\n    def _signal_shutdown(self,", "entropy": 45.83563363944406}
{"candidate": "        self._reactor = None\n\n    @defer.inlineCallbacks\n    def crawl(self, crawler_or_spidercls, *args, **kwargs):\n        \"\"\"\n        Run a crawler with the provided keyword arguments and return a\n        deferred that is fired when the crawl is finished.\n\n        If the crawler passed returns an item yield deferred, this method\n        will return such a deferred.\n\n        If the crawler passed", "entropy": 46.27584619223672}
{"candidate": "        self.spider_loader = _get_spider_loader(settings)\n        self.log_observer = ScrapyFileLogObserver(settings, _logs_path, None)\n        self.log_observer.start()\n\n    @defer.inlineCallbacks\n    def start(self):\n        \"\"\"\n        Starts the crawling process.\n\n        This method is called implicitly when the :class:`~scrapy", "entropy": 46.833191596561}
{"candidate": "\n    def start(self, stop_after_crawl=True):\n        \"\"\"\n        Runs the given crawlers (in the order they were given) and blocks until\n        they have finished crawling.\n\n        If ``stop_after_crawl`` is ``True`` (the default) this method will\n        stop() the reactor when crawlers are finished, otherwise it will be\n        left running.\n\n        If you want to crawl in a script and have it", "entropy": 57.57314010777489}
