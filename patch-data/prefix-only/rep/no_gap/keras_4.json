{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(grads,\n                                                    global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    def get_config(", "entropy": 2.3285374224649997}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(grads,\n                                                    global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    def get_config(", "entropy": 2.3285397061650004}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(grads,\n                                                    global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    def get_config(", "entropy": 2.3285397061650004}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(grads,\n                                                    global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    @property\n    def", "entropy": 3.002274587984}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(grads,\n                                                    global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    @property\n    def", "entropy": 3.007006348988}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(grads,\n                                                    global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    @property\n    def", "entropy": 3.013073826685999}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(grads, global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    def get_config(self):\n       ", "entropy": 3.3497510156429997}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        for grad, param in grads:\n            if grad is None:\n                continue\n            new_param = param - grad\n            self.updates.append(K.update(param, new_param))\n        return self.updates\n\n    def get", "entropy": 5.097632610913099}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(grads)\n        self.updates.append(opt_update)\n        return self.updates\n\n    @property\n    def weights(self):\n        raise NotImplementedError\n\n", "entropy": 5.299201250546998}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        for grad, param in grads:\n            if grad is None:\n                continue\n            new_param = self.optimizer.apply_gradients(\n                [(grad, param)],\n                global_step=self.iterations)\n            self.", "entropy": 5.783870165341501}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        opt_update = self.optimizer.apply_gradients(\n            grads, global_step=self.iterations)\n        self.updates.append(opt_update)\n        return self.updates\n\n    def get_config(self):", "entropy": 7.103496466063002}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        opt_update = self.optimizer.apply_gradients(grads,\n                                                    global_step=self.iterations,\n                                                    name=\"train\")\n        self.updates.append(opt_update)\n        return self.up", "entropy": 8.247677470583996}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        self.updates.extend([(p, self.optimizer.apply_gradients([g], [p]))\n                             for g, p in grads])\n        return self.updates\n\n    @property\n    def weights(self):\n", "entropy": 9.961431071963004}
{"candidate": "        self.optimizer.minimize(loss, var_list=params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        return self.updates\n\n    def get_config(self):\n        config = {}\n        base_config = super(TFOptimizer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()", "entropy": 10.609683063023704}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        for grad, param in grads:\n            if grad is None:\n                continue\n            new_p = self.optimizer.apply_gradients(\n                [(grad, param)],\n                global_step=self.iterations\n            )\n           ", "entropy": 10.7067901710287}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        for grad, var in grads:\n            self.updates.append(K.update(var, var - grad))\n        return self.updates\n\n    def get_config(self):\n        raise NotImplementedError\n\n\nclass SGD(", "entropy": 11.395646868146997}
{"candidate": "        grads_and_vars = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        self.weights.extend([self.iterations])\n\n        for grad, var in grads_and_vars:\n            if grad is None:\n                continue\n            scaled_grad = grad / (1. + self.iter", "entropy": 12.943007907252298}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [\n            K.update_add(self.iterations, 1)]\n        self.updates.extend(\n            [K.update(p, v) for p, v in zip(params, self.optimizer.apply_gradients(grads, global_step=self.iterations))])\n        return self", "entropy": 14.791500781919}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n        self.weights = []\n        for grad, param in grads:\n            if grad is not None:\n                p = K.variable(param, name='weight_')\n                self.weights.append(p)\n                new_p = p - grad", "entropy": 18.09780583568}
{"candidate": "        grads = self.optimizer.compute_gradients(loss, params)\n\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        for grad, var in grads:\n            if grad is None:\n                continue\n            updated_var = K.update(\n                var, self.optimizer.apply_gradients(\n                    [(grad, var)], global_step=self.iter", "entropy": 18.671181582459496}
