{"candidate": "        else:\n            tgt_values = target\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer), missing\n\n    @Appender(_index_shared_docs[\"get_indexer\"] % _index_doc_kwargs)\n    def get_indexer(self, target, method=None, limit=None, tolerance=None):", "entropy": 11.434834455921408}
{"candidate": "            (indexer, missing) = self._engine.get_indexer_non_unique(tgt_values)\n            return Index(indexer), missing\n        else:\n            (indexer, missing) = self._engine.get_indexer_non_unique(\n                com.values_from_object(target)\n            )\n            return Index(indexer), missing\n\n    _index_shared_docs[\n        \"get_", "entropy": 17.8760364616319}
{"candidate": "            if tgt_values.dtype != self.categories.dtype:\n                tgt_values = tgt_values.astype(self.categories.dtype)\n            target = ensure_index(target)\n        else:\n            tgt_values = target\n\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return ensure_platform_int(indexer", "entropy": 22.857144249428597}
{"candidate": "            target = target.categories.take(tgt_values)\n\n        rizer = self._engine.get_indexer_non_unique(target._ndarray_values)\n        indexer, missing = rizer.item_labels, rizer.missing_labels\n        return Index(indexer), missing\n\n    _index_shared_docs[\n        \"get_indexer\"\n    ] = \"\"\"\n        Compute indexer", "entropy": 30.2158850015132}
{"candidate": "            indexer = self.get_indexer(target.categories)\n            indexer = (indexer != -1).choose(indexer, tgt_values)\n            return indexer, np.arange(len(target))[indexer == -1]\n\n        target = _ensure_has_len(target)\n        if self.is_unique:\n            indexer = self._engine.get_indexer_non", "entropy": 32.31208552382057}
{"candidate": "            indexer, mask = self._engine.get_indexer_non_unique(tgt_values)\n            return indexer, mask\n\n        self._validate_index_level(0)\n        target = target._values\n        indexer, missing = self._engine.get_indexer_non_unique(target)\n        return _ensure_platform_int(indexer), missing\n\n    @Appender(Index.intersection.", "entropy": 32.45894482556489}
{"candidate": "            indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n            return ensure_platform_int(indexer), ensure_platform_int(missing)\n\n        if not is_dtype_equal(target.dtype, self.dtype):\n            # we need to cast the index to be the same type as the values\n            # otherwise we wont be able to properly compare\n            target = ensure_", "entropy": 32.9173868855262}
{"candidate": "            if self.categories.nlevels > 1:\n                tgt_values = tgt_values.codes\n            else:\n                tgt_values = tgt_values.codes.ravel()\n            indexer, missing = get_indexer_non_unique(\n                self.values, tgt_values, self.categories.categories\n            )\n        else:\n            indexer, missing = get_indexer_non", "entropy": 35.023494898264595}
{"candidate": "            if self.categories.is_unique and self.is_unique:\n                indexer = self._engine.get_indexer(target._values)\n            else:\n                indexer = (\n                    self._engine.get_indexer_non_unique(target._values)\n                    if self.categories.is_unique\n                    else self._engine.get_indexer_non_unique(target.categories)\n                )", "entropy": 36.07110393527801}
{"candidate": "            if self.categories.is_dtype_equal(target.categories):\n                categories = self.categories\n            else:\n                categories = target.categories.union(self.categories)\n            tgt_codes = categories.get_indexer(np.asarray(target))\n            indexer, missing = self.get_indexer_non_unique(tgt_codes)\n            return tgt_values.", "entropy": 38.180929823212495}
{"candidate": "            target = target.remove_unused_categories()\n            indexer = self._engine.get_indexer_non_unique(target._codes)\n            mask = indexer != -1\n            indexer = _ensure_int64(indexer)\n            indexer[indexer == -1] = -1\n            indexer[mask] = _maybe_rename_categories(tgt_values[mask], target)", "entropy": 41.68968558932939}
{"candidate": "            target = target._constructor(\n                tgt_values[\n                    self._engine.get_indexer_non_unique(tgt_values)\n                ].categories\n            )\n\n        target = Index(target, tupleize_cols=False)\n        indexer, missing = self._engine.get_indexer_non_unique(target._ndarray_values)\n        return Index(indexer), _ensure_platform", "entropy": 44.954868916789906}
{"candidate": "            tgt_categories = target.categories\n\n            if len(tgt_categories) < len(tgt_values):\n                all_categories = np.union1d(\n                    tgt_values, tgt_categories\n                )  # type: ignore[arg-type]\n                target = target.reindex(all_categories)\n                tgt_values = tgt_values.searchsorted(all", "entropy": 50.094655540687}
{"candidate": "        else:\n            tgt_values = target.values\n\n        # if target is included, directly return indexer\n        indexer, missing = self.get_indexer(target, method=\"pad\")\n        if len(missing) and self.is_unique and self.contains(target[missing]):\n            indexer, missing = self.get_indexer(target[missing], method=\"pad\")\n            indexer = np.con", "entropy": 51.390142481605}
{"candidate": "            return lib.fast_zip(self, tgt_values)\n\n        target = target.astype(object)\n        self_shape = len(self)\n\n        if self_shape == 0 or len(target) == 0:\n            indexer = np.repeat(-1, len(target))\n            missing = np.arange(len(target))\n            return indexer, missing\n\n        indexer = self._engine.get_index", "entropy": 52.26472793522121}
{"candidate": "        else:\n            # TODO: infer_dtype in here? can we infer if its a\n            # categorical even w/o the block?\n            tgt_values = com.values_from_object(target)\n\n        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)\n        return Index(indexer), missing\n\n    _index_shared_docs[\n        \"get_", "entropy": 57.82282071605282}
{"candidate": "            if target.categories is not self.categories:\n                raise ValueError(\n                    \"categories must match existing categories when appending\"\n                )\n            # otherwise, need to recompute\n        elif target.is_object() or target.is_interval():\n            tgt_values = np.asarray(np.array(target))\n        else:\n            tgt_values = target.values\n            if target.", "entropy": 58.34592657603799}
{"candidate": "            if self.dtype.categories.is_dtype_equal(target):\n                # *we* are the same type and *categories* are the same too\n                return self._engine.get_indexer_non_unique(\n                    tgt_values.codes, (tgt_values.codes != -1)\n                )\n\n            if self.dtype.categories.is_dtype_equal(target.dtype", "entropy": 62.423799410204}
{"candidate": "            tgt_codes = target.codes\n        else:\n            tgt_values = target.values\n            tgt_codes = (\n                None\n                if target.nlevels > 1\n                or not is_integer_dtype(tgt_values)\n                or is_integer_na(tgt_values)\n                else FACTORIZE_CACHE[tgt_values.astype(np.int64)]\n", "entropy": 63.087310471441604}
{"candidate": "            indexer, missing = self._engine.get_indexer(tgt_values)\n            if self.is_monotonic_increasing:\n                return indexer, missing\n            else:\n                # TODO: Varies per dtype\n                not_found = tgt_values.take(missing)\n                indexer = self.get_indexer_non_unique(not_found)\n                return indexer[0], index", "entropy": 67.7989878886667}
